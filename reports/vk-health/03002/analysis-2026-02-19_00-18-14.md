## VisionKing 03002 — System Analysis Report

**Timestamp:** 2026-02-19T03:15:01Z | **Snapshot delta:** 15 minutes

---

### SEVERITY: WARNING

---

### Executive Summary

The system is idle with no production activity — all Redis databases are empty, RabbitMQ queues show zero publish/deliver rates, and GPU utilization is 0% on both processing nodes. Infrastructure is stable with all 3 nodes reachable and all containers running. The primary concerns remain the persistent "database visionking does not exist" errors on vk01/vk02, the vk02 visualizer's recurring OOM pattern, and vk01's root disk at 81.6% capacity.

---

### Node Status

| Node | CPU | RAM | Disk (root) | Disk (img) | GPU Util | GPU Mem | GPU Temp | Uptime | Containers | Status |
|------|-----|-----|-------------|------------|----------|---------|----------|--------|------------|--------|
| vk01 | 2.9% | 13.1% | **81.6%** | 17% | 0% | 67.4% | 27°C | 2.7h | 27 running | OK |
| vk02 | 4.7% | 14.8% | 16.9% | 41% | 0% | 67.4% | 20°C | 5.1h | 27 running | OK |
| vk03 | 0.3% | 25.9% | 25.6% | N/A | N/A | N/A | N/A | 5.6h | 10 running | OK |

---

### Pipeline Health

**Camera Acquisition:** 8 camera containers (ca-sis-surface 1-8) running across vk01/vk02 with stable ~0.1 CPU each. No errors in logs. Acquiring frames but with nowhere to send them (Redis DB0 = 0 keys).

**Image Saver:** Healthy on both nodes (all sub-services: memory, rabbitmq, redis, storage = true).

**RabbitMQ:** All 6 queues (3 per node) at zero depth, zero publish/deliver rate, 1 consumer each. Pipeline is connected but idle.

**Inference:** Both containers running with models loaded (~5.97GB memory each) but at near-zero CPU. Models are warm but unused.

**Database Writers:** `dw-sis-surface` on both nodes at near-zero CPU. The vk02 writer has historical PostgreSQL connection errors to `10.244.70.50:2345` (Network unreachable) and RabbitMQ reconnection events, but these appear to have stabilized.

**No queue buildup, no restart indicators in this snapshot window.** Pipeline is dormant but structurally intact.

---

### PLC & Production State

**The production line is NOT running.**

- Redis DB1 (PLC state): **empty** on both vk01 and vk02 — no keys, no values
- Redis DB0 (frame cache): **0 keys** — no frames being cached
- Redis DB2 (camera config): **0 keys**
- Redis DB3 (settings): **empty**

The PLC monitors (`visionking-plc-monitor` on vk01, `pm-sis-surface` on vk02) are actively cycling through their read/write loops (~30ms per cycle) but reading empty PLC data blocks. The line is powered down or disconnected from the PLC. No speed, no material presence, no triggers firing.

---

### GUI & Infrastructure Status

| Endpoint | Status | Notes |
|----------|--------|-------|
| vk01 Frontend | 200 | OK |
| vk01 Portainer | 200 | OK |
| vk01 Redis Insight | 200 | OK |
| vk01 RabbitMQ UI | 200 | OK |
| vk01 pgAdmin | 302 | Redirect (login page) — normal |
| vk01 Grafana | 302 | Redirect (login page) — normal |
| vk02 Frontend | 200 | OK |
| vk02 Portainer | 200 | OK |
| vk02 Redis Insight | 200 | OK |
| vk02 RabbitMQ UI | 200 | OK |
| vk02 pgAdmin | 302 | Redirect (login page) — normal |
| vk02 Grafana | 302 | Redirect (login page) — normal |
| vk03 Frontend | 200 | OK |
| vk03 Portainer | 200 | OK |
| vk03 Grafana | 302 | Redirect (login page) — normal |

All web interfaces responding as expected. The 302 codes on pgAdmin and Grafana are normal authentication redirects.

---

### Changes Since Last Check (15-minute delta)

**Metrics shifts:**
- **vk01 GPU temp: 18°C → 27°C (+9°C)** — Significant swing upward after the 6°C drop noted in the previous report. The GPU is thermally cycling during idle, likely due to fan hysteresis (fan stops at low temp, GPU warms, fan kicks back on).
- **vk01 uptime: 8,646s → 9,546s** — Normal progression, no reboot.
- **vk02 uptime: 17,390s → 18,275s** — Normal progression, no reboot.
- **vk03 RAM: 26.1% → 25.9%** — Minor drop, within noise. Prometheus memory compacted (255MB → 237MB).
- **vk01 Prometheus memory: 789MB → 710MB** — Compaction occurred as expected within the oscillation band.
- **vk02 Prometheus memory: 690MB → 756MB** — Ingestion accumulation phase, ~4.4MB/min.
- **vk02 database-server memory: 1,082MB → 1,094MB (+12MB)** — PostgreSQL shared buffers growing slightly.

**Error logs:**
- **vk01 database-server:** New FATAL entries at 03:01:55 UTC — "database visionking does not exist" (2 entries). Confirms the 1/minute probe is still active.
- **vk02 database-server:** Same pattern at 02:39:03 UTC.
- **vk02 dw-sis-surface:** Error log window unchanged (same historical entries).
- **vk02 visionking-visualizer:** Error log window unchanged (same historical entries from Feb 4, 10, 18).

**No new containers, no container state changes, no new error patterns.**

---

### Trending Concerns (24h)

1. **vk01 root disk at 81.6% — flat but dangerously high.** Stable across all 15 samples at 81%, but leaves only ~18% headroom. Any burst of image saving or log accumulation could push this toward critical. The img_saved partition is only 17% used, so the pressure is on the root filesystem (likely Docker images/layers, logs, Prometheus TSDB).

2. **GPU memory locked at 67.4% with 0% utilization for 24+ hours.** ~6GB VRAM consumed per node with no inference activity. This is wasted capacity during what appears to be an extended idle period.

3. **vk01 CPU spiked to 7% once in the 24h window** (vs avg 3.6%). Likely a brief container activity burst, but worth noting as the only deviation from baseline.

4. **vk03 RAM is the highest at 25.9%** despite having the fewest containers (10 vs 27). The 5.78GB PostgreSQL data volume is the primary consumer.

---

### Error Log Analysis

**Cross-service correlation:**

1. **"database visionking does not exist"** — Appearing on both vk01 (03:01:55Z) and vk02 (02:39:03Z). The 22-minute offset between nodes suggests either two separate clients probing at different intervals, or a single client that staggers its connection attempts. Previously identified as originating from vk03 backends.

2. **vk02 `dw-sis-surface` PostgreSQL/RabbitMQ errors** — All historical (Feb 4, Feb 17, Feb 18). The service has since reconnected successfully (it's consuming from its RabbitMQ queue with 1 consumer active). These are boot-time transient errors, not ongoing.

3. **vk02 `ct-sis-surface` Redis IoError** — Historical `sw::redis::IoError` crashes (Feb 4, Feb 18) correlating with the grafana stack restarts on those dates. The service auto-recovered after Redis came back. The "Falha ao setar IP1!" errors suggest camera IP configuration fails when Redis is unreachable.

4. **vk02 `visionking-visualizer` OOM pattern** — Feb 4 (SIGKILL), Feb 10 (code 1 + reentrant logging crash), Feb 18 (code 1). Currently at 315MB, the lowest post-crash point. Memory will accumulate again.

5. **No cascade failures detected.** Errors are isolated to their respective services and don't propagate downstream.

---

### Recommendations

**Immediate (fix now):**
- None critical. System is idle and stable.

**Short-term (this week):**
- Fix the "database visionking does not exist" probe by correcting the database name in the vk03 backend connection strings.
- Add `--max-requests 1000 --max-requests-jitter 100` to vk02's `visionking-visualizer` gunicorn config before the next production shift to prevent OOM crashes.
- Investigate and free disk space on vk01's root partition (Docker image pruning, log rotation, Prometheus retention reduction).

**Long-term (architectural):**
- Align inference versions between vk01 (dev c014) and vk02 (prod c009) or formally document the A/B testing rationale.
- Implement automated disk space alerting at 85% threshold.
- Add HTTP health endpoints independent of SSH for monitoring resilience.

---

### NEW IMPROVEMENTS

1. **[INFO] vk01 GPU thermal cycling confirmed — 9°C swing in 15 minutes during idle** — Temperature went from 18°C to 27°C between snapshots, following the previous 6°C drop. This 15°C peak-to-trough oscillation in 30 minutes during constant zero-utilization load indicates the GPU fan has a wide hysteresis band (fan stops completely when cool, GPU heats passively, fan restarts aggressively). While not immediately damaging, sustained thermal cycling accelerates solder joint fatigue. Consider setting a minimum fan speed via `nvidia-smi` (`nvidia-smi -pl` or persistent fan curve) to keep the GPU at a steady ~25°C rather than oscillating 18-27°C.

2. **[INFO] vk02 `database-server` memory growing during idle — 1,082MB to 1,094MB (+12MB in 15 min)** — PostgreSQL on vk02 is accumulating shared buffer allocations despite zero active connections and no production writes. At 12MB per 15 minutes (~48MB/hour), this could indicate background autovacuum activity on stale tables, or uncommitted WAL segments. Run `SELECT pg_size_pretty(pg_database_size(datname)), datname FROM pg_database;` on vk02 port 2345 to check actual database sizes and `SELECT * FROM pg_stat_activity;` to identify any hidden connections not reflected in the active_connections metric.

3. **[SUGGESTION] vk02 `visionking-visualizer` idle CPU rate (0.48) is 6x higher than vk01's (0.08) — unchanged from previous report** — This disparity persists between snapshots and was previously flagged. The fact that vk02's rate is stable at 0.48 (not growing) suggests a steady-state polling loop rather than a runaway process. The most likely cause is a frontend WebSocket or SSE connection from the vk02 frontend container (which is on the same node) maintaining an active rendering/polling loop, while vk01's visualizer has no local frontend consumer. Verify whether the vk02 frontend's visualizer endpoint is being actively polled.
