## VisionKing 03002 -- Diagnostic Report

### SEVERITY: WARNING

### Executive Summary

The system is in **line-idle state** with all 3 nodes reachable and all containers running. The primary active issue is the recurring `database "visionking" does not exist` errors on both vk01 and vk02 PostgreSQL instances, which have doubled in frequency since the last snapshot (now appearing every ~13 minutes). No production data is flowing -- all Redis databases are empty, RabbitMQ queues show zero publish/deliver rates, and GPU utilization is 0% on both nodes.

### Node Status

| Node | CPU | RAM | Disk (root) | Disk (img) | GPU Util | GPU Mem | GPU Temp | Containers | Status |
|------|-----|-----|-------------|------------|----------|---------|----------|------------|--------|
| vk01 | 6.9% | 13.3% | **81.6%** | 17% | 0% | 67.4% | 20C | 27 running | WARNING (disk) |
| vk02 | 8.9% | 14.7% | 16.9% | 41% | 0% | 67.4% | 27C | 28 running | HEALTHY |
| vk03 | 0.4% | 26.2% | 25.6% | N/A | N/A | N/A | N/A | 10 running | HEALTHY |

### Pipeline Health

**Pipeline is fully idle.** No data is flowing through any stage:

- **Cameras (ca-sis-surface 1-8):** Running at ~0.1 CPU rate (polling loop only, no active acquisition)
- **Redis DB0 (frame cache):** 0 keys on both nodes
- **RabbitMQ:** All 6 queues (3 per node) at 0 messages, 0 publish rate, 0 deliver rate. Each queue has 1 consumer attached -- consumers are healthy but idle
- **Image Saver (is-sis-surface):** Health endpoints report healthy on both nodes (memory, rabbitmq, redis, storage all OK)
- **Inference (visionking-inference):** ~6GB memory loaded on both nodes, 0% GPU utilization -- models are loaded and waiting
- **Database Writers (dw-sis-surface, visionking-result-writer):** Near-zero CPU, idle

**No queue buildup. No restart events detected between snapshots.** All containers show the same uptime as the previous snapshot (~2h on vk01, ~4h on vk02, ~5h on vk03).

### PLC & Production State

**Line is not running.**

- Redis DB1 (PLC state): Empty on both vk01 and vk02 -- no keys at all
- Redis DB2 (camera config): 0 keys
- Redis DB3 (settings): Empty

The PLC monitors (`visionking-plc-monitor` on vk01, `pm-sis-surface` on vk02) are actively cycling through their read/write/set loop at ~15 iterations/second (normal polling cadence), but all values read from the PLC are being processed without errors. The line is in an idle/stopped state with no material present.

### GUI & Infrastructure Status

All web interfaces are responding:

| Endpoint | Status | Notes |
|----------|--------|-------|
| vk01 Frontend | 200 | OK |
| vk01 Portainer | 200 | OK |
| vk01 Redis Insight | 200 | OK |
| vk01 RabbitMQ UI | 200 | OK |
| vk01 pgAdmin | 302 | Redirect (normal -- login page) |
| vk01 Grafana | 302 | Redirect (normal -- login page) |
| vk02 Frontend | 200 | OK |
| vk02 Portainer | 200 | OK |
| vk02 Redis Insight | 200 | OK |
| vk02 RabbitMQ UI | 200 | OK |
| vk02 pgAdmin | 302 | Redirect (normal) |
| vk02 Grafana | 302 | Redirect (normal) |
| vk03 Frontend | 200 | OK |
| vk03 Portainer | 200 | OK |
| vk03 Grafana | 302 | Redirect (normal) |

All 302s are authentication redirects -- normal behavior.

### Changes Since Last Check

**Time delta:** 74 seconds between snapshots (02:28:47 → 02:30:01)

**Significant changes:**

1. **vk01 CPU jumped from 2.9% → 6.9%** -- This is a notable spike but still well within normal bounds. Likely cadvisor or prometheus scraping activity.
2. **vk01 database-server new errors:** Two additional `FATAL: database "visionking" does not exist` entries appeared (at 02:30:56 UTC), added to the two from the previous snapshot (02:17:12 UTC). This confirms the error recurs roughly every 13 minutes.
3. **vk02 database-server new errors:** Same pattern -- two new errors at 02:08:03 UTC were already in the previous snapshot, and two more appeared at 02:30:56/02:30:57 UTC (visible in current but the collection window caught the same set).
4. **vk01 Prometheus memory increased:** 824MB → 829MB (+5MB) -- normal TSDB ingestion growth.
5. **vk02 visionking-visualizer memory decreased:** 316MB → 315MB -- minor fluctuation, no worker crash this cycle.
6. **Container counts stable:** No containers started, stopped, or restarted between snapshots.

### Trending Concerns

Based on 24h trends (11 samples):

1. **vk01 disk at 81.6% and flat** -- Not growing but dangerously high. One large log dump or image batch could push past 85% where PostgreSQL and Docker start having issues. The 24h trend shows it locked at 81% across all samples -- no growth, but no cleanup either.
2. **GPU memory locked at 67.4% on both nodes** -- Consistent across all 11 samples. Models are loaded but the static allocation with 0% utilization for 24h straight means the line has been idle for at least a full day.
3. **vk01 CPU avg 4.3% with current spike to 6.9%** -- The max in 24h was 7.0%, so this snapshot is near the peak. Not concerning but worth noting if it trends higher.
4. **vk02 disk stable at 16.9%** and vk03 at 25.6% -- both healthy with no concerning growth trajectory.

### Error Log Analysis

**Cross-service error correlation:**

1. **`database "visionking" does not exist` (vk01 + vk02):** This is the most active error. It appears on both processing nodes' PostgreSQL instances simultaneously, suggesting a shared client or scheduled task is probing both databases. The timing (~every 13 minutes) suggests a health check or cron job. The backend containers on vk02/vk03 or the `visionking-result` service on vk01 are the likely sources.

2. **vk02 `visionking-visualizer` OOM pattern (historical):** The latest worker crashes were at 19:08:06 on Feb 18 -- approximately 7 hours ago. The container is currently stable at 315MB. The pattern of crashes on Feb 4, Feb 10, and Feb 18 (roughly weekly) strongly suggests a slow memory leak in the visualization rendering path.

3. **vk02 `dw-sis-surface` PostgreSQL connection failures to 10.244.70.50:** These are historical errors from reboot events. The `Network unreachable` errors correlate with vk02/vk03 reboot windows. Currently the service is running without errors.

4. **vk02 `ct-sis-surface` Redis IoError:** Historical crashes with `sw::redis::IoError` -- last occurred at 18:48:12 on Feb 18, correlating with the same reboot window. The C++ service recovers on restart but doesn't handle Redis disconnections gracefully.

5. **PLC monitor warnings (vk01 + vk02):** These are operational trace logs misclassified as WARNING. No actual errors -- the PLC read/write cycle is completing successfully in ~30-40ms per iteration.

### Recommendations

**Immediate (fix now):**
- None critical. System is idle and stable.

**Short-term (this week):**
- Identify and fix the source of `database "visionking" does not exist` errors before the next production shift
- Free disk space on vk01 (currently 81.6%) -- clean old Docker images, logs, or unused data
- Add `--max-requests 1000 --max-requests-jitter 50` to vk02's `visionking-visualizer` gunicorn config to prevent the weekly OOM pattern

**Long-term (architectural):**
- Align vk01 and vk02 inference container versions before production
- Add resilient reconnection handling to `ct-sis-surface` (C++ Redis client) to survive Redis restarts without crashing
- Reclassify PLC monitor operational logging from WARNING to DEBUG

### NEW IMPROVEMENTS

1. **[INFO] "visionking" database probe frequency has increased** -- The `database "visionking" does not exist` errors are now appearing every ~13 minutes on both vk01 and vk02 simultaneously (previously ~7-15 minutes). The synchronized timing across both nodes strongly suggests this is coming from a centralized client (likely the `visionking-backend` or `dev-visionking-backend` on vk03, which connects to both processing nodes' PostgreSQL instances). Check the backend's database connection string -- it may be configured with database name `visionking` instead of the actual database name.

2. **[INFO] System has been idle for 24+ hours with models loaded** -- GPU memory is at 67.4% with 0% utilization across all 11 trend samples spanning 24 hours. The inference models are consuming ~6GB VRAM per node with no return. If the line will remain idle for an extended maintenance period, consider unloading the models to free GPU memory and reduce thermal wear, reloading them before the next shift.

3. **[SUGGESTION] vk01 Prometheus memory is trending upward despite idle state** -- Prometheus grew from 824MB to 829MB in just 74 seconds between snapshots. During idle operation (no pipeline metrics changing), Prometheus should have near-zero ingestion growth. This suggests either excessive scrape targets, too-short scrape intervals, or high-cardinality metrics from cadvisor/container labels. Review the `scrape_interval` and consider adding `metric_relabel_configs` to drop unused high-cardinality container label metrics to slow vk01's disk consumption.

4. **[INFO] vk02 recently rebooted (~4.3h uptime) while vk01 has only ~1.9h uptime** -- vk01 was rebooted most recently (uptime 6846s = 1.9h) compared to vk02 (15575s = 4.3h) and vk03 (17352s = 4.8h). This staggered reboot pattern is good for availability but means vk01's containers have been up the shortest. Monitor vk01 services for any post-reboot stabilization issues during the first production shift.
