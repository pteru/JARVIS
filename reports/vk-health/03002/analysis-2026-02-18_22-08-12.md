

# VisionKing 03002 Health Report — 2026-02-19 00:47 UTC

### SEVERITY: WARNING

### Executive Summary

The deployment is **idle** — all 3 nodes are reachable, all pipeline containers are running, but there is zero production throughput (0 keys in Redis, 0 queue messages, 0 publish/deliver rate). vk01 recently rebooted (uptime ~12 min) and has recovered, but its disk remains critically full at **81.6%**. The vk03 backend is experiencing persistent Redis connectivity failures to vk02 (10.244.70.26:4000), suggesting a network or firewall issue that emerged during the vk01/vk02 reboot window.

### Node Status

| Node | CPU | RAM | Disk | GPU Util | GPU Mem | GPU Temp | Containers | Uptime | Status |
|------|-----|-----|------|----------|---------|----------|------------|--------|--------|
| vk01 | 2.9% | 12.9% | **81.6%** | 0% | 67.4% | 26°C | 27 running, 1 restarting | ~12 min | WARNING |
| vk02 | 4.7% | 14.7% | 16.9% | 0% | 67.4% | 24°C | 29 running, 1 restarting | ~2.6 hrs | OK |
| vk03 | 0.6% | 26.4% | 25.6% | N/A | N/A | N/A | 10 running | ~3.1 hrs | WARNING |

### Pipeline Health

**Pipeline is IDLE.** No frames flowing through the system:

- **Redis DB0 (frame cache):** 0 keys on both vk01 and vk02 — no frames buffered
- **Redis DB1 (PLC):** empty keys on both nodes — no PLC tag data present
- **Redis DB2 (camera):** 0 keys — no camera metadata
- **Redis DB3 (settings):** empty
- **RabbitMQ queues:** All 3 queues on both nodes (dw-sis-surface-queue, is-sis-surface-queue, rc-sis-surface-queue) have 0 messages, 1 consumer each, 0 publish/deliver rate — consumers are connected but nothing is flowing
- **Image Saver:** healthy on both nodes (memory, rabbitmq, redis, storage all green)

**Container stability:**
- `intel-gpu-exporter` in restart loop on both vk01 and vk02 — no Intel GPU on these NVIDIA machines, this is a misconfiguration (known issue, not new)
- All pipeline services (`ca-sis-surface-*`, `is-sis-surface`, `visionking-inference`, `dw-sis-surface`, `ct-sis-surface`, `visionking-result`) are running

**Key observation:** vk01 just rebooted (~12 min uptime vs previous snapshot's ~36 sec), and all containers recovered. The previous snapshot caught it mid-boot which explains the null CPU/GPU readings then.

### PLC & Production State

**The production line is NOT running.**

Redis DB1 (PLC state) is completely empty on both vk01 and vk02 — no keys at all. This means:
- No speed data → line is stopped
- No part presence signals → no material on the line
- No tracking triggers → camera acquisition is not being triggered by the PLC
- The PLC monitors (`visionking-plc-monitor` on vk01, `pm-sis-surface` on vk02) are running their read/write loops (visible in logs: readBar/setBar cycling every ~30ms) but there's no active bar data being stored to Redis

This is consistent with a **night shift shutdown** (snapshot at 00:47 UTC = ~21:47 local -03).

### GUI & Infrastructure Status

| Endpoint | Status | Notes |
|----------|--------|-------|
| vk01 Frontend | 200 | OK |
| vk01 Portainer | 200 | OK |
| vk01 Redis Insight | 200 | OK |
| vk01 RabbitMQ UI | 200 | OK |
| vk01 PgAdmin | 302 | OK (redirect to login) |
| vk01 Grafana | 302 | OK (redirect to login) |
| vk02 Frontend | 200 | OK |
| vk02 Portainer | 200 | OK |
| vk02 Redis Insight | 200 | OK |
| vk02 RabbitMQ UI | 200 | OK |
| vk02 PgAdmin | 302 | OK (redirect to login) |
| vk02 Grafana | 302 | OK (redirect to login) |
| vk03 Frontend | 200 | OK |
| vk03 Portainer | 200 | OK |
| **vk03 PgAdmin** | **0** | **DOWN** |
| vk03 Grafana | 302 | OK (redirect to login) |
| **vk01 Visualizer** | **0** | **DOWN** |
| **vk02 Visualizer** | **0** | **DOWN** |

### Changes Since Last Check

| Metric | Previous (00:36) | Current (00:47) | Delta |
|--------|-------------------|------------------|-------|
| vk01 uptime | 36 sec | 711 sec (+12 min) | Booted and stabilized |
| vk01 CPU | null (booting) | 2.9% | Now reporting |
| vk01 GPU util | null | 0% | Now reporting |
| vk01 GPU mem | null | 67.4% | Model loaded |
| vk01 RAM | 12.5% | 12.9% | +0.4% (containers warming) |
| vk02 uptime | 8780s | 9455s | +675s (stable) |
| vk02 visualizer CPU | 0.471 | 0.484 | Slight increase |
| vk03 RAM | 25.3% | 26.4% | +1.1% — Prometheus data accumulating |
| vk03 backend Redis errors | Active (ECONNREFUSED) | Same pattern | **Persistent** — not resolved |

**Notable:** The vk01 `database-server` error changed from connection terminations (admin shutdown during reboot) to `database "visionking" does not exist` — this indicates PostgreSQL is up but the expected database hasn't been created on vk01. This is the same error pattern on vk02's database-server.

### Trending Concerns

Based on 24h trends (6 samples):

1. **vk01 disk at 81.6% — flat but critical.** Not growing (steady at 81% across all samples), but dangerously close to the threshold where PostgreSQL WAL and container overlays will fail. vk02 is at 17% for comparison — this asymmetry suggests vk01 has accumulated data/logs that vk02 hasn't.

2. **GPU memory locked at 67.4% on both nodes** across all samples with 0% utilization. The inference models are loaded into VRAM but idle. This is expected during no-production, but the 67% baseline leaves only 33% headroom when production starts.

3. **vk03 RAM trending up:** 25% → 26.4%. The database-server container on vk03 has 5.78GB of data (visible in docker Size). With only a 111MB container memory footprint, the host RAM increase is likely from Prometheus data retention growing.

### Error Log Analysis

**Cross-correlated error patterns:**

1. **vk03 Backend → vk02 Redis connectivity failure (CASCADE)**
   - `visionking-backend` on vk03 logs rapid-fire `ECONNREFUSED 10.244.70.26:4000` errors
   - This progressed to `EHOSTUNREACH` (network-level) then back to `ECONNREFUSED`
   - **Root cause:** vk02 KeyDB (cache container) was temporarily unreachable during vk02's grafana stack restart (~20:19). The backend hasn't recovered its Redis connection.
   - **Impact:** vk03 dashboard cannot display live PLC/camera data from vk02's Redis

2. **"database visionking does not exist" on vk01 and vk02**
   - Both PostgreSQL instances reject connections to a database named "visionking"
   - The actual database likely has a different name (e.g., `sis_surface` or similar)
   - Something is periodically attempting to connect to a non-existent DB — likely a health check or misconfigured service

3. **Camera acquisition LUT errors (vk01, all 4 cameras)**
   - `Erro ao definir o valor da feature 'LUTValueAll'` on Hikrobot cameras DA3488406-DA3488409
   - Followed by "Thread bad performance" — a startup-time error when LUT (Look-Up Table) configuration fails
   - These are boot-time errors that appeared after vk01's reboot. The cameras are running despite this.

4. **vk02 Visualizer OOM kills (historical, recurring)**
   - Workers being SIGKILL'd with "Perhaps out of memory?" on Feb 4 and Feb 10
   - The visualizer on vk02 is currently using 414MB with 0.48 CPU rate — the highest CPU consumer after camera acquisition
   - This service has a memory leak pattern that surfaces periodically

5. **vk02 `dw-sis-surface` PostgreSQL "Network unreachable" to 10.244.70.50:2345**
   - This targets vk01's PostgreSQL. The errors span from Feb 4 to Feb 21 (timestamps in logs include future dates — likely timezone/clock issues)
   - Combined with vk01's frequent reboots, this writer has intermittent connectivity to its target DB

### Recommendations

**Immediate:**
- Investigate vk01 disk usage (81.6%). Identify what's consuming space — likely old container images, logs, or MongoDB data (284MB visible in mongo container alone, plus 928MB database-server). A `docker system prune` and log rotation could recover significant space.

**Short-term (this week):**
- Fix the `database "visionking" does not exist` error — identify which service is connecting with the wrong database name and correct its configuration
- Restart vk03's `visionking-backend` to clear the stale Redis connection state to vk02
- Remove `intel-gpu-exporter` from the grafana docker-compose on vk01/vk02 — it will never find an Intel GPU on NVIDIA machines and wastes restart cycles

**Long-term (architectural):**
- Add connection retry/reconnect logic to the vk03 backend's Redis client — a single connectivity blip shouldn't require a container restart to recover
- Implement log rotation for the PLC monitors (`visionking-plc-monitor`, `pm-sis-surface`) — they emit WARNING-level logs for every single PLC read cycle (~30ms), generating enormous log volumes

### NEW IMPROVEMENTS

1. **[WARNING] vk03 backend has persistent broken Redis connection to vk02** — The `visionking-backend` on vk03 is stuck in a Redis reconnect loop to `10.244.70.26:4000` since the ~21:40 reboot window. The connection progressed through ECONNREFUSED → EHOSTUNREACH → ECONNREFUSED, indicating vk02's KeyDB went down during its grafana stack restart and the NestJS Redis client didn't auto-recover. Restart the vk03 backend container, and file a backlog task to add resilient Redis reconnection logic.

2. **[WARNING] "database visionking does not exist" errors on both vk01 and vk02 PostgreSQL** — An unknown client is periodically attempting to connect to a database named `visionking` which doesn't exist. This generates FATAL errors in PostgreSQL logs every ~7-15 minutes. Identify the source (likely the `visionking-backend` or `dev-visionking-backend` container using a wrong DB name in its connection string) and fix the configuration.

3. **[SUGGESTION] vk02 `visionking-visualizer` shows recurring OOM pattern** — Workers are being SIGKILL'd with memory exhaustion (Feb 4, Feb 10, Feb 18). The container currently uses 414MB. Add `--max-requests` and `--max-requests-jitter` to the gunicorn config to periodically recycle workers before they accumulate too much memory, or increase the container's memory limit.

4. **[INFO] vk01 recently rebooted and GPU model is loaded but idle** — GPU memory is at 67.4% with 0% utilization on both nodes. The inference models loaded successfully post-reboot. Camera LUT configuration warnings during boot (`LUTValueAll: Don't know how to set value from string`) do not appear to prevent camera operation but should be investigated to ensure image quality is not degraded.

5. **[SUGGESTION] Clock synchronization issue on vk02** — The `dw-sis-surface` logs on vk02 contain timestamps from `2026-02-21 18:00:01` (3 days in the future), mixed with legitimate timestamps. This suggests the system clock drifted or NTP is misconfigured. Verify `timedatectl` and NTP sync status on vk02, as timestamp accuracy is critical for production defect traceability.
