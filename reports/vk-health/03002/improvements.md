# VisionKing 03002 -- Live Improvements Report


## 2026-02-18 21:53 -- WARNING
1. **[WARNING] SSH unreachable on all 3 nodes** — Investigate SSH daemon status, firewall rules, and key/credential validity. Use Portainer web consoles (all responding) as an alternative entry point to diagnose. Consider adding a secondary access method (e.g., WireGuard VPN or Tailscale) for redundancy.
2. **[WARNING] vk03 pgAdmin is down (status 0)** — Check the pgAdmin container via vk03 Portainer. If PostgreSQL itself is also down, the entire dashboard stack is at risk (though frontend responding at 200 suggests the DB is likely fine).
3. **[INFO] vk01/vk02 Visualizer services are down (status 0)** — Clarify whether these are intentionally not deployed in 03002 or represent failed containers. If not needed, remove from the monitored endpoints list to avoid false warnings.
4. **[SUGGESTION] Implement HTTP health endpoints as SSH-independent monitoring** — The current monitoring is entirely SSH-dependent. Adding `/health` or `/ready` endpoints to key pipeline services would allow health checks even when SSH is unavailable, providing a more resilient observability layer.

## 2026-02-18 22:08 -- WARNING
1. **[WARNING] vk03 backend has persistent broken Redis connection to vk02** — The `visionking-backend` on vk03 is stuck in a Redis reconnect loop to `10.244.70.26:4000` since the ~21:40 reboot window. The connection progressed through ECONNREFUSED → EHOSTUNREACH → ECONNREFUSED, indicating vk02's KeyDB went down during its grafana stack restart and the NestJS Redis client didn't auto-recover. Restart the vk03 backend container, and file a backlog task to add resilient Redis reconnection logic.
2. **[WARNING] "database visionking does not exist" errors on both vk01 and vk02 PostgreSQL** — An unknown client is periodically attempting to connect to a database named `visionking` which doesn't exist. This generates FATAL errors in PostgreSQL logs every ~7-15 minutes. Identify the source (likely the `visionking-backend` or `dev-visionking-backend` container using a wrong DB name in its connection string) and fix the configuration.
3. **[SUGGESTION] vk02 `visionking-visualizer` shows recurring OOM pattern** — Workers are being SIGKILL'd with memory exhaustion (Feb 4, Feb 10, Feb 18). The container currently uses 414MB. Add `--max-requests` and `--max-requests-jitter` to the gunicorn config to periodically recycle workers before they accumulate too much memory, or increase the container's memory limit.
4. **[INFO] vk01 recently rebooted and GPU model is loaded but idle** — GPU memory is at 67.4% with 0% utilization on both nodes. The inference models loaded successfully post-reboot. Camera LUT configuration warnings during boot (`LUTValueAll: Don't know how to set value from string`) do not appear to prevent camera operation but should be investigated to ensure image quality is not degraded.
5. **[SUGGESTION] Clock synchronization issue on vk02** — The `dw-sis-surface` logs on vk02 contain timestamps from `2026-02-21 18:00:01` (3 days in the future), mixed with legitimate timestamps. This suggests the system clock drifted or NTP is misconfigured. Verify `timedatectl` and NTP sync status on vk02, as timestamp accuracy is critical for production defect traceability.

## 2026-02-18 23:02 -- WARNING
1. **[INFO] `intel-gpu-exporter` was removed from vk01 between snapshots** — This crash-looping container (searching for non-existent Intel i915 GPU on an NVIDIA machine) has been cleaned up. Verify it was also removed from the `docker-compose.yml` on vk01 so it doesn't return on next reboot. Also check vk02 — it was still crash-looping there in the previous snapshot but is absent from the current container *metrics* (though not confirmed removed from Docker).
2. **[SUGGESTION] PLC monitor WARNING log level is too verbose for normal operation** — Both `visionking-plc-monitor` (vk01) and `pm-sis-surface` (vk02) emit ~50 WARNING-level log lines per PLC cycle (~300ms). This is normal operational flow logging (`Inicio readBar`, `Fim readBar`, etc.) misclassified as WARNING. At ~170 log lines/second, this generates unnecessary log volume. Reclassify these to DEBUG level and reserve WARNING for actual anomalies.
3. **[INFO] vk02 `visionking-visualizer` memory dropped 100MB between snapshots (414MB → 315MB)** — This confirms the OOM recycling pattern: a worker accumulated memory, was killed, and respawned lighter. The gunicorn `--max-requests` recommendation from the previous report should be prioritized before the next production shift.
4. **[SUGGESTION] vk03 `database-server` container has 5.78GB disk footprint** — The PostgreSQL data volume on vk03 (`downloads_data...`) is growing. At 25.6% disk usage this isn't critical, but vk03 has no automated cleanup. Consider implementing WAL archiving or `pg_dump` rotation to prevent vk03 from following vk01's disk trajectory.

## 2026-02-18 23:18 -- WARNING
1. **[INFO] vk01 Prometheus memory dropped 93MB between snapshots (787MB → 694MB)** — This is likely a TSDB head compaction or block retention event. While normal, Prometheus on vk01 at ~694MB is the single largest non-inference container. If vk01 disk pressure worsens, consider reducing `--storage.tsdb.retention.time` or moving Prometheus to vk02 which has 83% disk headroom.
2. **[INFO] Version inconsistency between vk01 and vk02 inference containers** — vk01 runs `dev-visionking-inference:c014` (dev tag, newer commit `a8399db`) while vk02 runs `visionking-inference:c009` (prod tag, older commit `3a0eb21`). This means the two processing nodes are running different inference model versions. If this is intentional A/B testing, document it. If not, align both nodes to the same version before the next production shift to ensure consistent defect detection.
3. **[SUGGESTION] vk02 has additional pipeline services not present on vk01** — vk02 runs `visionking-length-measure`, `rc-sis-surface`, and `pm-sis-surface` that vk01 does not have (vk01 has `visionking-plc-monitor` and `dw-sis-surface` instead of `pm-sis-surface` and `rc-sis-surface`). This asymmetric deployment means the two nodes are not interchangeable — if vk02 goes down, length measurement and the `rc` pipeline stage are lost with no redundancy. Document the asymmetry and evaluate whether critical services should be mirrored.
4. **[INFO] All Redis databases empty across both nodes (DB0-DB3)** — This is expected during line downtime, but confirms there is no persistent state surviving across production sessions in Redis. If the line starts unexpectedly, all camera configs (DB2), PLC state (DB1), and settings (DB3) will need to be re-populated. Verify that the initialization sequence handles cold-start from empty Redis gracefully.

## 2026-02-18 23:33 -- WARNING
1. **[INFO] "visionking" database probe frequency has increased** -- The `database "visionking" does not exist` errors are now appearing every ~13 minutes on both vk01 and vk02 simultaneously (previously ~7-15 minutes). The synchronized timing across both nodes strongly suggests this is coming from a centralized client (likely the `visionking-backend` or `dev-visionking-backend` on vk03, which connects to both processing nodes' PostgreSQL instances). Check the backend's database connection string -- it may be configured with database name `visionking` instead of the actual database name.
2. **[INFO] System has been idle for 24+ hours with models loaded** -- GPU memory is at 67.4% with 0% utilization across all 11 trend samples spanning 24 hours. The inference models are consuming ~6GB VRAM per node with no return. If the line will remain idle for an extended maintenance period, consider unloading the models to free GPU memory and reduce thermal wear, reloading them before the next shift.
3. **[SUGGESTION] vk01 Prometheus memory is trending upward despite idle state** -- Prometheus grew from 824MB to 829MB in just 74 seconds between snapshots. During idle operation (no pipeline metrics changing), Prometheus should have near-zero ingestion growth. This suggests either excessive scrape targets, too-short scrape intervals, or high-cardinality metrics from cadvisor/container labels. Review the `scrape_interval` and consider adding `metric_relabel_configs` to drop unused high-cardinality container label metrics to slow vk01's disk consumption.
4. **[INFO] vk02 recently rebooted (~4.3h uptime) while vk01 has only ~1.9h uptime** -- vk01 was rebooted most recently (uptime 6846s = 1.9h) compared to vk02 (15575s = 4.3h) and vk03 (17352s = 4.8h). This staggered reboot pattern is good for availability but means vk01's containers have been up the shortest. Monitor vk01 services for any post-reboot stabilization issues during the first production shift.

## 2026-02-18 23:36 -- WARNING
1. **[INFO] vk01 Prometheus memory self-corrected via TSDB compaction** — Prometheus dropped from 829MB to 634MB (~195MB freed) between snapshots, confirming the earlier upward trend was pre-compaction ingestion buffer. The compaction occurred naturally within the 3-minute window. No action needed, but this establishes a baseline: Prometheus on vk01 oscillates between ~630-830MB on its compaction cycle. If it exceeds 900MB without compacting, investigate retention settings.
2. **[INFO] vk02 `visionking-visualizer` memory is re-accumulating post-crash** — After the Feb 18 19:08 worker crash that dropped memory to ~315MB, the container has already climbed back to 334MB (up 19MB in ~7 hours of idle operation). At this rate (~2.7MB/hour), it will return to the ~400MB crash threshold in approximately 24 hours even without production load. This confirms the memory leak is in the gunicorn worker initialization/idle path, not just request handling. The `--max-requests` fix will help during production, but the underlying leak in the idle code path should also be investigated.
3. **[SUGGESTION] vk03 database-server write amplification is unusually high** — The vk03 PostgreSQL container shows 5.78GB disk footprint (virtual 8.06GB) while reporting 0 active connections and 0 db_size_bytes. The 0-byte DB size reading may indicate the monitoring query is connecting to the wrong database or the `visionking` database doesn't exist on vk03 either. Meanwhile, the container's 5.78GB diff layer suggests significant WAL or temp file accumulation. Run `SELECT pg_database_size(current_database())` directly on vk03's port 2347 to verify actual data size and check for orphaned WAL segments.
4. **[SUGGESTION] Standardize Docker Compose version across nodes** — vk01 uses Compose v2.32.4, vk02 uses v2.29.7, and vk03 uses v2.31.0. While functionally compatible, version drift makes troubleshooting compose-specific behavior differences harder. Align all nodes to the same Compose version during the next maintenance window.

## 2026-02-19 02:43 -- WARNING
1. **[INFO] vk02 visualizer memory dropped 18MB in 10 minutes during idle** — Memory went from 334MB to 316MB without production load, suggesting a silent worker recycle not captured in error logs. Add `--access-logfile -` and `--error-logfile -` flags to gunicorn to capture worker lifecycle events and correlate with memory drops.
2. **[INFO] PostgreSQL "visionking" probe rate confirmed at exactly 1 per minute per node** — 2 new FATAL entries per 10-minute delta on both nodes simultaneously, confirming a 1-minute interval health check from a single centralized client. The source is definitively the vk03 backends. Check NestJS TypeORM/Prisma configuration on both `visionking-backend` (port 8000) and `dev-visionking-backend` (port 5777) for a `database: "visionking"` setting.
3. **[SUGGESTION] vk01 Prometheus memory oscillation is widening** — Ingestion rate measured at 13.4MB/min (634MB → 768MB in 10 minutes). During production (when container metrics change more frequently), the peak could exceed 1GB. Consider setting `--storage.tsdb.max-block-duration=2h` to trigger more frequent compactions and cap memory usage on the disk-constrained vk01.

## 2026-02-19 00:03 -- WARNING
1. **[INFO] vk01 GPU temperature dropped 6°C in 17 minutes (24°C → 18°C) during idle** — This unusually rapid cooling suggests either the GPU fan profile is aggressive in idle mode, or ambient temperature changed (night cooling). While not problematic, a 6°C swing in 17 minutes during constant idle load is worth noting. If temperature oscillates this much, it may cause thermal cycling stress on the GPU. Monitor for GPU throttling events when production resumes to verify thermal management is adequate under load.
2. **[INFO] vk01 `database-server` FATAL errors disappeared from current error log window** — The `database "visionking" does not exist` errors that appeared in the previous snapshot's error logs for vk01 are absent from the current snapshot, while vk02's are still present. This likely means vk01's error log window rotated past those entries (they were from 02:30-02:35 UTC, now 17 minutes later). However, it could also indicate the probing client targeting vk01 stopped while vk02's continues — worth verifying whether the vk03 backend connects to both nodes or only vk02.
3. **[SUGGESTION] vk02 `visionking-visualizer` idle CPU consumption is disproportionately high** — At 0.49 CPU rate during complete idle (no frames, no users), this container is consuming 6x more CPU than vk01's identical visualizer (0.08 CPU). This 6x differential with no production load suggests a busy-loop, excessive polling, or a stuck worker in the vk02 instance. Investigate with `top` inside the container or check for a websocket reconnection loop that may be spinning without backoff.

## 2026-02-19 00:18 -- WARNING
1. **[INFO] vk01 GPU thermal cycling confirmed — 9°C swing in 15 minutes during idle** — Temperature went from 18°C to 27°C between snapshots, following the previous 6°C drop. This 15°C peak-to-trough oscillation in 30 minutes during constant zero-utilization load indicates the GPU fan has a wide hysteresis band (fan stops completely when cool, GPU heats passively, fan restarts aggressively). While not immediately damaging, sustained thermal cycling accelerates solder joint fatigue. Consider setting a minimum fan speed via `nvidia-smi` (`nvidia-smi -pl` or persistent fan curve) to keep the GPU at a steady ~25°C rather than oscillating 18-27°C.
2. **[INFO] vk02 `database-server` memory growing during idle — 1,082MB to 1,094MB (+12MB in 15 min)** — PostgreSQL on vk02 is accumulating shared buffer allocations despite zero active connections and no production writes. At 12MB per 15 minutes (~48MB/hour), this could indicate background autovacuum activity on stale tables, or uncommitted WAL segments. Run `SELECT pg_size_pretty(pg_database_size(datname)), datname FROM pg_database;` on vk02 port 2345 to check actual database sizes and `SELECT * FROM pg_stat_activity;` to identify any hidden connections not reflected in the active_connections metric.
3. **[SUGGESTION] vk02 `visionking-visualizer` idle CPU rate (0.48) is 6x higher than vk01's (0.08) — unchanged from previous report** — This disparity persists between snapshots and was previously flagged. The fact that vk02's rate is stable at 0.48 (not growing) suggests a steady-state polling loop rather than a runaway process. The most likely cause is a frontend WebSocket or SSE connection from the vk02 frontend container (which is on the same node) maintaining an active rendering/polling loop, while vk01's visualizer has no local frontend consumer. Verify whether the vk02 frontend's visualizer endpoint is being actively polled.

## 2026-02-19 00:33 -- WARNING
1. **[WARNING] vk01 GPU metrics (memory, temperature) have gone null — potential GPU exporter or driver failure** — Between the 03:15 and 03:30 snapshots, vk01's `gpu_mem_pct` dropped from 67.4% to null and `gpu_temp_c` from 27°C to null, while `gpu_util_pct` remains at 0%. This indicates the `nvidia-gpu-exporter` (DCGM exporter) on vk01 can no longer read GPU telemetry. Check `docker logs nvidia-gpu-exporter` on vk01 for DCGM errors, and run `nvidia-smi` directly on the host to verify the GPU driver is still responsive. If the driver has entered a fault state, the inference container (which is still reporting 5.97GB memory usage via Docker stats) may also be unable to run inference when production resumes. This should be verified before the next shift.
2. **[INFO] vk01 and vk02 database-server error log PIDs are advancing, confirming active probing** — vk01 PIDs jumped from 13168/13175 (03:01 UTC) to 14518/14525 (03:16 UTC), a delta of ~1350 PIDs in 15 minutes. Each probe spawns 2 short-lived PostgreSQL backend processes. This PID advancement rate (~90 PIDs/min) is dominated by the `visionking` database probes. While PostgreSQL handles this gracefully, the PID churn adds unnecessary entries to `pg_stat_activity` history and bloats CSV logs if enabled. Fixing the connection string on vk03 will eliminate this overhead.
3. **[INFO] vk01 cadvisor memory dropped significantly — 195MB to 168MB (-27MB)** — This 14% reduction in cadvisor's memory footprint may indicate cadvisor performed an internal cache eviction or garbage collection. Given that cadvisor is scraping 27 containers on vk01, its 168MB footprint is reasonable but worth watching — cadvisor memory growth on large container counts is a known issue. If it climbs back above 200MB, consider adding `--docker_only=true` and `--disable_metrics=referenced_memory,cpu_topology,resctrl,hugetlb` flags to reduce its collection scope.

## 2026-02-19 00:48 -- WARNING
1. **[WARNING] vk01 GPU telemetry blackout now sustained for 30+ minutes across two snapshot cycles** -- The null readings for `gpu_mem_pct` and `gpu_temp_c` on vk01 have persisted from the previous snapshot (03:30) through the current one (03:45), while `gpu_util_pct` still reports 0%. This is no longer a transient scrape failure -- the DCGM exporter has lost communication with the GPU driver. The Docker-level container stats still show `visionking-inference` at 5.97GB memory, meaning the container process is alive but its GPU access state is unknown. Before the next production shift: (a) check `docker logs nvidia-gpu-exporter` on vk01 for DCGM errors, (b) run `nvidia-smi` on the host to test driver responsiveness, and (c) if the driver is unresponsive, a host reboot may be required to recover the GPU.
2. **[INFO] vk03 Prometheus memory grew 17% in 15 minutes during idle (242MB → 283MB)** -- While vk01 and vk02 Prometheus instances compacted (both decreased), vk03's grew by 41MB. vk03 Prometheus is scraping only 10 containers (vs 27-29 on vk01/vk02), so this growth rate is disproportionate. Check whether vk03's Prometheus has different retention or scrape interval settings than the other nodes. If vk03's `scrape_interval` is shorter (e.g., 5s vs 15s), it would explain the faster ingestion despite fewer targets.
3. **[INFO] vk02 GPU temperature stabilized at 23°C -- no thermal cycling observed** -- Unlike vk01's previously reported 18-27°C oscillation, vk02's GPU temperature moved only 2°C (21→23°C) between snapshots. This differential behavior between identical hardware suggests vk01's fan control issue may be node-specific (BIOS fan curve, physical fan obstruction, or thermal paste degradation on vk01). When vk01's GPU telemetry recovers, compare fan RPM readings between nodes to isolate the cause.
4. **[SUGGESTION] vk02 `is-sis-surface` (image saver) container has accumulated 120MB of writable layer data** -- The container's `Size` field shows `120MB (virtual 295MB)`, up from `120MB (virtual 294MB)` in the previous snapshot. This writable layer growth during idle suggests the image saver is writing temporary files or logs inside the container filesystem rather than to mounted volumes. Over time this bloats the container's diff layer and slows `docker commit`/migration operations. Mount an external volume for any temp file operations, or add a periodic cleanup inside the container.

## 2026-02-22 20:16 -- WARNING
1. **[WARNING] `bar_uuid` and `part_uuid` are systematically 'empty' during active production, causing 100% inspection data loss to PostgreSQL** — Both `dw-sis-surface` (ERROR-level) and `visionking-result-writer` (WARNING-level) are rejecting every message because the bar/part UUID fields contain the literal string `'empty'`. This is happening while bars are actively being inspected (F32331400601-F32331400804). The `visionking-result` service CAN identify bars (it writes pass/fail to Redis by bar ID), but this identification is not reaching the database writer message path. Investigate the message enrichment step between inference output and database writer input — the component responsible for attaching `bar_uuid` to inference results is either misconfigured or the PLC tracking trigger that associates frames to bars is not firing for the database writer queue.
2. **[WARNING] Redis hash `DB_Speed` missing field `Velocidade_Tratada` — inference defaulting to 100% frame sampling** — The inference `batch_processor.py:233` logs that the speed field expected from PLC is absent, forcing it to process every frame rather than subsampling based on line speed. During active production at 2631 mm/min, this means significantly more GPU work than necessary. Verify the `visionking-plc-monitor` is writing speed data to the correct Redis DB and key name. The PLC monitor logs show `writeBar` completing successfully, but the data may be going to a different Redis database or key than what inference expects.
3. **[INFO] vk01 GPU recovered from previous telemetry blackout — now reporting 36°C and 67.4% memory** — The GPU null readings reported in the 00:33-00:48 UTC improvements have resolved. The GPU is now healthy and actively processing inference. The DCGM exporter recovered without intervention, likely after a driver-level transient. No immediate action needed, but this confirms the GPU driver on vk01 has an intermittent communication issue with DCGM worth monitoring.
4. **[INFO] vk01 `database-server` (PostgreSQL) memory grew 300MB in 2.5 hours during production (15.8GB → 16.1GB)** — This is the largest memory consumer on vk01 at 16GB. The growth during active production is expected (shared buffers, work_mem for queries), but at this trajectory it will consume significant host RAM over a full production shift. Monitor for OOM pressure on vk01 given inference (15.1GB) + PostgreSQL (16.1GB) already total 31.2GB.
5. **[SUGGESTION] Inference image loading race condition is pervasive across all 4 cameras** — The `File is empty` warnings appear for all camera IDs (DA3488406, DA3488407, DA3488408, DA3488409) at ~3-4 second intervals. While the 3-retry mechanism handles this, the frequency suggests a systematic timing issue: the image saver notifies inference via RabbitMQ before the `.bin` file is fully flushed to disk. Adding an `fsync` or a brief write-completion signal before publishing to the queue would eliminate these retries and reduce inference latency by ~0.2-0.4s per affected frame.
6. **[INFO] vk02 is fully dark — reachable at network level but running zero containers** — In the previous analysis cycle vk02 had running containers. It now shows empty container lists, null Prometheus host metrics, and persistent ssh_error on health endpoints. Only the RabbitMQ queues on vk02 still show consumers (likely connected from vk01's consumers to vk02's broker). This effectively makes the deployment single-node (vk01) for all processing, with no redundancy for cameras, inference, or database writing.

## 2026-02-23 01:00 -- WARNING
1. **[INFO] vk01 reboot freed 29GB of accumulated process memory — confirms memory management gaps across multiple services** — PostgreSQL (15.5GB freed), inference (9.1GB freed), and image saver (4.5GB freed) all released massive memory upon restart. The inference container dropped from 15.1GB to 5.97GB, meaning 9.1GB was runtime accumulation (model weights are ~5.97GB baseline). PostgreSQL going from 16.7GB to 1.17GB indicates `shared_buffers` or `work_mem` is set too high, or the connection pooling is not releasing memory. Set `shared_buffers` to a fixed ceiling (e.g., 4GB) and add `idle_in_transaction_session_timeout` to prevent long-lived backend processes from hoarding memory.
2. **[WARNING] vk01 `is-sis-surface` (image saver) leaked 4.46GB of memory during one production shift** — This container went from 78.7MB (cold start) to 4.54GB during production and back to 78.7MB after reboot. A 57x memory inflation during a single shift indicates the image saver is retaining frame data in memory (likely an unbounded buffer or cache that doesn't evict). This is the third-largest memory consumer on vk01 after PostgreSQL and inference. Add memory limits to this container and investigate the frame caching logic for missing eviction/TTL.
3. **[INFO] All production errors were transient to the last shift and cleared by reboot — no persistent corruption detected** — The empty `bar_uuid` errors, image file race conditions, and missing `Velocidade_Tratada` key all ceased cleanly after the reboot. RabbitMQ queues are at 0 with no poison messages. This confirms the error patterns are runtime state issues (likely triggered by a specific PLC/line condition during startup) rather than data corruption or software bugs that persist across restarts.
4. **[INFO] Camera agents consume 4x more memory during production than at cold start (43-50MB idle vs 167-232MB active)** — The 4 camera acquisition containers each grew ~4x during the last production run. While this is a smaller scale than other leaks, multiplied by 4 cameras it totals ~700MB of additional memory during production. Monitor whether this growth is bounded (stabilizes after warmup) or continues to accumulate over multi-hour shifts.
5. **[SUGGESTION] vk01 `visionking-plc-monitor` PLC polling cycle is missing `writeBar` calls during idle — verify the PLC connection is live** — In the previous snapshot (during production), the PLC monitor cycle included `writeBar` and `alarms` steps. In the current idle snapshot, the cycle is only `setBar → readBar → redisRead → setBar` with no `writeBar` or `alarms`. While this could simply mean there's nothing to write, it could also indicate the PLC communication link has been torn down. Before production resumes, verify the PLC monitor can reach the PLC controller and that the bar tracking DB blocks are accessible.
6. **[INFO] vk01 Prometheus memory reset from ~860MB to 818MB after reboot — establishes clean baseline** — The reboot gave Prometheus a fresh TSDB. Monitor the growth rate during the next production shift to establish a production-load memory trajectory from this known starting point. If it exceeds 1GB during a single shift, the scrape interval and metric cardinality reductions recommended in previous reports should be prioritized.

## 2026-02-23 05:44 -- WARNING
1. **[WARNING] `is-sis-surface` (image saver) is leaking ~2.9GB/hour during IDLE — 10.7GB and growing with zero production load** — This container grew from 4.2GB to 10.7GB (+6.5GB) in the 2.25 hours between snapshots, with no frames being processed (Redis DB0 has 0 keys, RabbitMQ has 0 publish rate). This is worse than the previous production-shift leak (which was ~4.5GB over a full shift) — the idle leak rate now exceeds the production leak rate. The container's writable layer also grew from 114MB to 125MB. This suggests a background process inside the image saver (possibly a monitoring thread, health check response accumulator, or unclosed file handles from the previous production run) is continuously allocating memory. Restart the container immediately and impose a `mem_limit: 4g` in its compose file. Then instrument the container with `tracemalloc` or `/proc/PID/smaps` analysis to identify the leaking allocation path.
2. **[WARNING] vk01 PostgreSQL resumed aggressive idle memory growth — 3.0GB to 5.2GB (+74%) in 2.25 hours with zero active connections** — Despite the reboot 7 hours ago resetting PostgreSQL to 1.2GB, it has already climbed to 5.2GB with `active_connections: 0`. The previous cycle saw it reach 16.7GB before reboot. At the current growth rate (~1GB/hour), it will return to 16GB within 11 hours. The `shared_buffers` recommendation from previous reports has not been implemented. Additionally, the periodic "database visionking does not exist" probes (confirmed at 1/minute from vk03) are spawning and killing backend processes, which may be contributing to memory fragmentation. Prioritize setting `shared_buffers = 2GB` and `effective_cache_size = 4GB` in `postgresql.conf`.
3. **[INFO] vk01 `visionking-inference` memory baseline established post-reboot — 8.8GB is the warm idle footprint** — The inference container settled at 8.8GB after startup (up from 5.97GB cold start seen immediately post-reboot, to 8.7GB at the previous snapshot, to 8.8GB now). This +84MB growth over 2.25 hours of idle is minimal and expected (model warm-up, CUDA context). The previous production shift inflated it to 15.1GB, meaning ~6.3GB was runtime accumulation. Monitor whether it stays at ~8.8GB through idle, and track the growth rate once production resumes to distinguish model warm-up from actual leaks.
4. **[INFO] `visionking-visualizer` memory growing 4MB/hour during idle (358MB → 367MB)** — This confirms the idle-path memory leak previously reported for vk02's visualizer also exists on vk01. At 4MB/hour, it will reach the ~400MB OOM-crash threshold observed on vk02 in approximately 8 hours. The `--max-requests` gunicorn fix has still not been deployed. Both nodes' visualizer containers will likely need restarts during the next production shift.
5. **[SUGGESTION] RabbitMQ delivery tags on `dw-sis-surface` jumped from ~48k to ~160k — 112,000 messages consumed and discarded in one production window** — This quantifies the data loss from the `bar_uuid='empty'` bug: approximately 112,000 inspection result messages were pulled from the queue and thrown away. At ~8 messages per batch and ~4 cameras, this represents roughly 28,000 frame-level inspection results that were computed by GPU inference but never persisted. For a production traceability system in steel manufacturing, this is a compliance risk. Consider adding a dead-letter queue so discarded messages are retained for later reprocessing once the UUID enrichment bug is fixed, rather than being permanently lost.
