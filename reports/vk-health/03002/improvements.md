# VisionKing 03002 -- Live Improvements Report


## 2026-02-18 21:53 -- WARNING
1. **[WARNING] SSH unreachable on all 3 nodes** — Investigate SSH daemon status, firewall rules, and key/credential validity. Use Portainer web consoles (all responding) as an alternative entry point to diagnose. Consider adding a secondary access method (e.g., WireGuard VPN or Tailscale) for redundancy.
2. **[WARNING] vk03 pgAdmin is down (status 0)** — Check the pgAdmin container via vk03 Portainer. If PostgreSQL itself is also down, the entire dashboard stack is at risk (though frontend responding at 200 suggests the DB is likely fine).
3. **[INFO] vk01/vk02 Visualizer services are down (status 0)** — Clarify whether these are intentionally not deployed in 03002 or represent failed containers. If not needed, remove from the monitored endpoints list to avoid false warnings.
4. **[SUGGESTION] Implement HTTP health endpoints as SSH-independent monitoring** — The current monitoring is entirely SSH-dependent. Adding `/health` or `/ready` endpoints to key pipeline services would allow health checks even when SSH is unavailable, providing a more resilient observability layer.

## 2026-02-18 22:08 -- WARNING
1. **[WARNING] vk03 backend has persistent broken Redis connection to vk02** — The `visionking-backend` on vk03 is stuck in a Redis reconnect loop to `10.244.70.26:4000` since the ~21:40 reboot window. The connection progressed through ECONNREFUSED → EHOSTUNREACH → ECONNREFUSED, indicating vk02's KeyDB went down during its grafana stack restart and the NestJS Redis client didn't auto-recover. Restart the vk03 backend container, and file a backlog task to add resilient Redis reconnection logic.
2. **[WARNING] "database visionking does not exist" errors on both vk01 and vk02 PostgreSQL** — An unknown client is periodically attempting to connect to a database named `visionking` which doesn't exist. This generates FATAL errors in PostgreSQL logs every ~7-15 minutes. Identify the source (likely the `visionking-backend` or `dev-visionking-backend` container using a wrong DB name in its connection string) and fix the configuration.
3. **[SUGGESTION] vk02 `visionking-visualizer` shows recurring OOM pattern** — Workers are being SIGKILL'd with memory exhaustion (Feb 4, Feb 10, Feb 18). The container currently uses 414MB. Add `--max-requests` and `--max-requests-jitter` to the gunicorn config to periodically recycle workers before they accumulate too much memory, or increase the container's memory limit.
4. **[INFO] vk01 recently rebooted and GPU model is loaded but idle** — GPU memory is at 67.4% with 0% utilization on both nodes. The inference models loaded successfully post-reboot. Camera LUT configuration warnings during boot (`LUTValueAll: Don't know how to set value from string`) do not appear to prevent camera operation but should be investigated to ensure image quality is not degraded.
5. **[SUGGESTION] Clock synchronization issue on vk02** — The `dw-sis-surface` logs on vk02 contain timestamps from `2026-02-21 18:00:01` (3 days in the future), mixed with legitimate timestamps. This suggests the system clock drifted or NTP is misconfigured. Verify `timedatectl` and NTP sync status on vk02, as timestamp accuracy is critical for production defect traceability.

## 2026-02-18 23:02 -- WARNING
1. **[INFO] `intel-gpu-exporter` was removed from vk01 between snapshots** — This crash-looping container (searching for non-existent Intel i915 GPU on an NVIDIA machine) has been cleaned up. Verify it was also removed from the `docker-compose.yml` on vk01 so it doesn't return on next reboot. Also check vk02 — it was still crash-looping there in the previous snapshot but is absent from the current container *metrics* (though not confirmed removed from Docker).
2. **[SUGGESTION] PLC monitor WARNING log level is too verbose for normal operation** — Both `visionking-plc-monitor` (vk01) and `pm-sis-surface` (vk02) emit ~50 WARNING-level log lines per PLC cycle (~300ms). This is normal operational flow logging (`Inicio readBar`, `Fim readBar`, etc.) misclassified as WARNING. At ~170 log lines/second, this generates unnecessary log volume. Reclassify these to DEBUG level and reserve WARNING for actual anomalies.
3. **[INFO] vk02 `visionking-visualizer` memory dropped 100MB between snapshots (414MB → 315MB)** — This confirms the OOM recycling pattern: a worker accumulated memory, was killed, and respawned lighter. The gunicorn `--max-requests` recommendation from the previous report should be prioritized before the next production shift.
4. **[SUGGESTION] vk03 `database-server` container has 5.78GB disk footprint** — The PostgreSQL data volume on vk03 (`downloads_data...`) is growing. At 25.6% disk usage this isn't critical, but vk03 has no automated cleanup. Consider implementing WAL archiving or `pg_dump` rotation to prevent vk03 from following vk01's disk trajectory.

## 2026-02-18 23:18 -- WARNING
1. **[INFO] vk01 Prometheus memory dropped 93MB between snapshots (787MB → 694MB)** — This is likely a TSDB head compaction or block retention event. While normal, Prometheus on vk01 at ~694MB is the single largest non-inference container. If vk01 disk pressure worsens, consider reducing `--storage.tsdb.retention.time` or moving Prometheus to vk02 which has 83% disk headroom.
2. **[INFO] Version inconsistency between vk01 and vk02 inference containers** — vk01 runs `dev-visionking-inference:c014` (dev tag, newer commit `a8399db`) while vk02 runs `visionking-inference:c009` (prod tag, older commit `3a0eb21`). This means the two processing nodes are running different inference model versions. If this is intentional A/B testing, document it. If not, align both nodes to the same version before the next production shift to ensure consistent defect detection.
3. **[SUGGESTION] vk02 has additional pipeline services not present on vk01** — vk02 runs `visionking-length-measure`, `rc-sis-surface`, and `pm-sis-surface` that vk01 does not have (vk01 has `visionking-plc-monitor` and `dw-sis-surface` instead of `pm-sis-surface` and `rc-sis-surface`). This asymmetric deployment means the two nodes are not interchangeable — if vk02 goes down, length measurement and the `rc` pipeline stage are lost with no redundancy. Document the asymmetry and evaluate whether critical services should be mirrored.
4. **[INFO] All Redis databases empty across both nodes (DB0-DB3)** — This is expected during line downtime, but confirms there is no persistent state surviving across production sessions in Redis. If the line starts unexpectedly, all camera configs (DB2), PLC state (DB1), and settings (DB3) will need to be re-populated. Verify that the initialization sequence handles cold-start from empty Redis gracefully.

## 2026-02-18 23:33 -- WARNING
1. **[INFO] "visionking" database probe frequency has increased** -- The `database "visionking" does not exist` errors are now appearing every ~13 minutes on both vk01 and vk02 simultaneously (previously ~7-15 minutes). The synchronized timing across both nodes strongly suggests this is coming from a centralized client (likely the `visionking-backend` or `dev-visionking-backend` on vk03, which connects to both processing nodes' PostgreSQL instances). Check the backend's database connection string -- it may be configured with database name `visionking` instead of the actual database name.
2. **[INFO] System has been idle for 24+ hours with models loaded** -- GPU memory is at 67.4% with 0% utilization across all 11 trend samples spanning 24 hours. The inference models are consuming ~6GB VRAM per node with no return. If the line will remain idle for an extended maintenance period, consider unloading the models to free GPU memory and reduce thermal wear, reloading them before the next shift.
3. **[SUGGESTION] vk01 Prometheus memory is trending upward despite idle state** -- Prometheus grew from 824MB to 829MB in just 74 seconds between snapshots. During idle operation (no pipeline metrics changing), Prometheus should have near-zero ingestion growth. This suggests either excessive scrape targets, too-short scrape intervals, or high-cardinality metrics from cadvisor/container labels. Review the `scrape_interval` and consider adding `metric_relabel_configs` to drop unused high-cardinality container label metrics to slow vk01's disk consumption.
4. **[INFO] vk02 recently rebooted (~4.3h uptime) while vk01 has only ~1.9h uptime** -- vk01 was rebooted most recently (uptime 6846s = 1.9h) compared to vk02 (15575s = 4.3h) and vk03 (17352s = 4.8h). This staggered reboot pattern is good for availability but means vk01's containers have been up the shortest. Monitor vk01 services for any post-reboot stabilization issues during the first production shift.

## 2026-02-18 23:36 -- WARNING
1. **[INFO] vk01 Prometheus memory self-corrected via TSDB compaction** — Prometheus dropped from 829MB to 634MB (~195MB freed) between snapshots, confirming the earlier upward trend was pre-compaction ingestion buffer. The compaction occurred naturally within the 3-minute window. No action needed, but this establishes a baseline: Prometheus on vk01 oscillates between ~630-830MB on its compaction cycle. If it exceeds 900MB without compacting, investigate retention settings.
2. **[INFO] vk02 `visionking-visualizer` memory is re-accumulating post-crash** — After the Feb 18 19:08 worker crash that dropped memory to ~315MB, the container has already climbed back to 334MB (up 19MB in ~7 hours of idle operation). At this rate (~2.7MB/hour), it will return to the ~400MB crash threshold in approximately 24 hours even without production load. This confirms the memory leak is in the gunicorn worker initialization/idle path, not just request handling. The `--max-requests` fix will help during production, but the underlying leak in the idle code path should also be investigated.
3. **[SUGGESTION] vk03 database-server write amplification is unusually high** — The vk03 PostgreSQL container shows 5.78GB disk footprint (virtual 8.06GB) while reporting 0 active connections and 0 db_size_bytes. The 0-byte DB size reading may indicate the monitoring query is connecting to the wrong database or the `visionking` database doesn't exist on vk03 either. Meanwhile, the container's 5.78GB diff layer suggests significant WAL or temp file accumulation. Run `SELECT pg_database_size(current_database())` directly on vk03's port 2347 to verify actual data size and check for orphaned WAL segments.
4. **[SUGGESTION] Standardize Docker Compose version across nodes** — vk01 uses Compose v2.32.4, vk02 uses v2.29.7, and vk03 uses v2.31.0. While functionally compatible, version drift makes troubleshooting compose-specific behavior differences harder. Align all nodes to the same Compose version during the next maintenance window.
