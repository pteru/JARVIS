# VisionKing 03002 -- Live Improvements Report


## 2026-02-18 21:53 -- WARNING
1. **[WARNING] SSH unreachable on all 3 nodes** — Investigate SSH daemon status, firewall rules, and key/credential validity. Use Portainer web consoles (all responding) as an alternative entry point to diagnose. Consider adding a secondary access method (e.g., WireGuard VPN or Tailscale) for redundancy.
2. **[WARNING] vk03 pgAdmin is down (status 0)** — Check the pgAdmin container via vk03 Portainer. If PostgreSQL itself is also down, the entire dashboard stack is at risk (though frontend responding at 200 suggests the DB is likely fine).
3. **[INFO] vk01/vk02 Visualizer services are down (status 0)** — Clarify whether these are intentionally not deployed in 03002 or represent failed containers. If not needed, remove from the monitored endpoints list to avoid false warnings.
4. **[SUGGESTION] Implement HTTP health endpoints as SSH-independent monitoring** — The current monitoring is entirely SSH-dependent. Adding `/health` or `/ready` endpoints to key pipeline services would allow health checks even when SSH is unavailable, providing a more resilient observability layer.

## 2026-02-18 22:08 -- WARNING
1. **[WARNING] vk03 backend has persistent broken Redis connection to vk02** — The `visionking-backend` on vk03 is stuck in a Redis reconnect loop to `10.244.70.26:4000` since the ~21:40 reboot window. The connection progressed through ECONNREFUSED → EHOSTUNREACH → ECONNREFUSED, indicating vk02's KeyDB went down during its grafana stack restart and the NestJS Redis client didn't auto-recover. Restart the vk03 backend container, and file a backlog task to add resilient Redis reconnection logic.
2. **[WARNING] "database visionking does not exist" errors on both vk01 and vk02 PostgreSQL** — An unknown client is periodically attempting to connect to a database named `visionking` which doesn't exist. This generates FATAL errors in PostgreSQL logs every ~7-15 minutes. Identify the source (likely the `visionking-backend` or `dev-visionking-backend` container using a wrong DB name in its connection string) and fix the configuration.
3. **[SUGGESTION] vk02 `visionking-visualizer` shows recurring OOM pattern** — Workers are being SIGKILL'd with memory exhaustion (Feb 4, Feb 10, Feb 18). The container currently uses 414MB. Add `--max-requests` and `--max-requests-jitter` to the gunicorn config to periodically recycle workers before they accumulate too much memory, or increase the container's memory limit.
4. **[INFO] vk01 recently rebooted and GPU model is loaded but idle** — GPU memory is at 67.4% with 0% utilization on both nodes. The inference models loaded successfully post-reboot. Camera LUT configuration warnings during boot (`LUTValueAll: Don't know how to set value from string`) do not appear to prevent camera operation but should be investigated to ensure image quality is not degraded.
5. **[SUGGESTION] Clock synchronization issue on vk02** — The `dw-sis-surface` logs on vk02 contain timestamps from `2026-02-21 18:00:01` (3 days in the future), mixed with legitimate timestamps. This suggests the system clock drifted or NTP is misconfigured. Verify `timedatectl` and NTP sync status on vk02, as timestamp accuracy is critical for production defect traceability.

## 2026-02-18 23:02 -- WARNING
1. **[INFO] `intel-gpu-exporter` was removed from vk01 between snapshots** — This crash-looping container (searching for non-existent Intel i915 GPU on an NVIDIA machine) has been cleaned up. Verify it was also removed from the `docker-compose.yml` on vk01 so it doesn't return on next reboot. Also check vk02 — it was still crash-looping there in the previous snapshot but is absent from the current container *metrics* (though not confirmed removed from Docker).
2. **[SUGGESTION] PLC monitor WARNING log level is too verbose for normal operation** — Both `visionking-plc-monitor` (vk01) and `pm-sis-surface` (vk02) emit ~50 WARNING-level log lines per PLC cycle (~300ms). This is normal operational flow logging (`Inicio readBar`, `Fim readBar`, etc.) misclassified as WARNING. At ~170 log lines/second, this generates unnecessary log volume. Reclassify these to DEBUG level and reserve WARNING for actual anomalies.
3. **[INFO] vk02 `visionking-visualizer` memory dropped 100MB between snapshots (414MB → 315MB)** — This confirms the OOM recycling pattern: a worker accumulated memory, was killed, and respawned lighter. The gunicorn `--max-requests` recommendation from the previous report should be prioritized before the next production shift.
4. **[SUGGESTION] vk03 `database-server` container has 5.78GB disk footprint** — The PostgreSQL data volume on vk03 (`downloads_data...`) is growing. At 25.6% disk usage this isn't critical, but vk03 has no automated cleanup. Consider implementing WAL archiving or `pg_dump` rotation to prevent vk03 from following vk01's disk trajectory.

## 2026-02-18 23:18 -- WARNING
1. **[INFO] vk01 Prometheus memory dropped 93MB between snapshots (787MB → 694MB)** — This is likely a TSDB head compaction or block retention event. While normal, Prometheus on vk01 at ~694MB is the single largest non-inference container. If vk01 disk pressure worsens, consider reducing `--storage.tsdb.retention.time` or moving Prometheus to vk02 which has 83% disk headroom.
2. **[INFO] Version inconsistency between vk01 and vk02 inference containers** — vk01 runs `dev-visionking-inference:c014` (dev tag, newer commit `a8399db`) while vk02 runs `visionking-inference:c009` (prod tag, older commit `3a0eb21`). This means the two processing nodes are running different inference model versions. If this is intentional A/B testing, document it. If not, align both nodes to the same version before the next production shift to ensure consistent defect detection.
3. **[SUGGESTION] vk02 has additional pipeline services not present on vk01** — vk02 runs `visionking-length-measure`, `rc-sis-surface`, and `pm-sis-surface` that vk01 does not have (vk01 has `visionking-plc-monitor` and `dw-sis-surface` instead of `pm-sis-surface` and `rc-sis-surface`). This asymmetric deployment means the two nodes are not interchangeable — if vk02 goes down, length measurement and the `rc` pipeline stage are lost with no redundancy. Document the asymmetry and evaluate whether critical services should be mirrored.
4. **[INFO] All Redis databases empty across both nodes (DB0-DB3)** — This is expected during line downtime, but confirms there is no persistent state surviving across production sessions in Redis. If the line starts unexpectedly, all camera configs (DB2), PLC state (DB1), and settings (DB3) will need to be re-populated. Verify that the initialization sequence handles cold-start from empty Redis gracefully.

## 2026-02-18 23:33 -- WARNING
1. **[INFO] "visionking" database probe frequency has increased** -- The `database "visionking" does not exist` errors are now appearing every ~13 minutes on both vk01 and vk02 simultaneously (previously ~7-15 minutes). The synchronized timing across both nodes strongly suggests this is coming from a centralized client (likely the `visionking-backend` or `dev-visionking-backend` on vk03, which connects to both processing nodes' PostgreSQL instances). Check the backend's database connection string -- it may be configured with database name `visionking` instead of the actual database name.
2. **[INFO] System has been idle for 24+ hours with models loaded** -- GPU memory is at 67.4% with 0% utilization across all 11 trend samples spanning 24 hours. The inference models are consuming ~6GB VRAM per node with no return. If the line will remain idle for an extended maintenance period, consider unloading the models to free GPU memory and reduce thermal wear, reloading them before the next shift.
3. **[SUGGESTION] vk01 Prometheus memory is trending upward despite idle state** -- Prometheus grew from 824MB to 829MB in just 74 seconds between snapshots. During idle operation (no pipeline metrics changing), Prometheus should have near-zero ingestion growth. This suggests either excessive scrape targets, too-short scrape intervals, or high-cardinality metrics from cadvisor/container labels. Review the `scrape_interval` and consider adding `metric_relabel_configs` to drop unused high-cardinality container label metrics to slow vk01's disk consumption.
4. **[INFO] vk02 recently rebooted (~4.3h uptime) while vk01 has only ~1.9h uptime** -- vk01 was rebooted most recently (uptime 6846s = 1.9h) compared to vk02 (15575s = 4.3h) and vk03 (17352s = 4.8h). This staggered reboot pattern is good for availability but means vk01's containers have been up the shortest. Monitor vk01 services for any post-reboot stabilization issues during the first production shift.

## 2026-02-18 23:36 -- WARNING
1. **[INFO] vk01 Prometheus memory self-corrected via TSDB compaction** — Prometheus dropped from 829MB to 634MB (~195MB freed) between snapshots, confirming the earlier upward trend was pre-compaction ingestion buffer. The compaction occurred naturally within the 3-minute window. No action needed, but this establishes a baseline: Prometheus on vk01 oscillates between ~630-830MB on its compaction cycle. If it exceeds 900MB without compacting, investigate retention settings.
2. **[INFO] vk02 `visionking-visualizer` memory is re-accumulating post-crash** — After the Feb 18 19:08 worker crash that dropped memory to ~315MB, the container has already climbed back to 334MB (up 19MB in ~7 hours of idle operation). At this rate (~2.7MB/hour), it will return to the ~400MB crash threshold in approximately 24 hours even without production load. This confirms the memory leak is in the gunicorn worker initialization/idle path, not just request handling. The `--max-requests` fix will help during production, but the underlying leak in the idle code path should also be investigated.
3. **[SUGGESTION] vk03 database-server write amplification is unusually high** — The vk03 PostgreSQL container shows 5.78GB disk footprint (virtual 8.06GB) while reporting 0 active connections and 0 db_size_bytes. The 0-byte DB size reading may indicate the monitoring query is connecting to the wrong database or the `visionking` database doesn't exist on vk03 either. Meanwhile, the container's 5.78GB diff layer suggests significant WAL or temp file accumulation. Run `SELECT pg_database_size(current_database())` directly on vk03's port 2347 to verify actual data size and check for orphaned WAL segments.
4. **[SUGGESTION] Standardize Docker Compose version across nodes** — vk01 uses Compose v2.32.4, vk02 uses v2.29.7, and vk03 uses v2.31.0. While functionally compatible, version drift makes troubleshooting compose-specific behavior differences harder. Align all nodes to the same Compose version during the next maintenance window.

## 2026-02-19 02:43 -- WARNING
1. **[INFO] vk02 visualizer memory dropped 18MB in 10 minutes during idle** — Memory went from 334MB to 316MB without production load, suggesting a silent worker recycle not captured in error logs. Add `--access-logfile -` and `--error-logfile -` flags to gunicorn to capture worker lifecycle events and correlate with memory drops.
2. **[INFO] PostgreSQL "visionking" probe rate confirmed at exactly 1 per minute per node** — 2 new FATAL entries per 10-minute delta on both nodes simultaneously, confirming a 1-minute interval health check from a single centralized client. The source is definitively the vk03 backends. Check NestJS TypeORM/Prisma configuration on both `visionking-backend` (port 8000) and `dev-visionking-backend` (port 5777) for a `database: "visionking"` setting.
3. **[SUGGESTION] vk01 Prometheus memory oscillation is widening** — Ingestion rate measured at 13.4MB/min (634MB → 768MB in 10 minutes). During production (when container metrics change more frequently), the peak could exceed 1GB. Consider setting `--storage.tsdb.max-block-duration=2h` to trigger more frequent compactions and cap memory usage on the disk-constrained vk01.

## 2026-02-19 00:03 -- WARNING
1. **[INFO] vk01 GPU temperature dropped 6°C in 17 minutes (24°C → 18°C) during idle** — This unusually rapid cooling suggests either the GPU fan profile is aggressive in idle mode, or ambient temperature changed (night cooling). While not problematic, a 6°C swing in 17 minutes during constant idle load is worth noting. If temperature oscillates this much, it may cause thermal cycling stress on the GPU. Monitor for GPU throttling events when production resumes to verify thermal management is adequate under load.
2. **[INFO] vk01 `database-server` FATAL errors disappeared from current error log window** — The `database "visionking" does not exist` errors that appeared in the previous snapshot's error logs for vk01 are absent from the current snapshot, while vk02's are still present. This likely means vk01's error log window rotated past those entries (they were from 02:30-02:35 UTC, now 17 minutes later). However, it could also indicate the probing client targeting vk01 stopped while vk02's continues — worth verifying whether the vk03 backend connects to both nodes or only vk02.
3. **[SUGGESTION] vk02 `visionking-visualizer` idle CPU consumption is disproportionately high** — At 0.49 CPU rate during complete idle (no frames, no users), this container is consuming 6x more CPU than vk01's identical visualizer (0.08 CPU). This 6x differential with no production load suggests a busy-loop, excessive polling, or a stuck worker in the vk02 instance. Investigate with `top` inside the container or check for a websocket reconnection loop that may be spinning without backoff.

## 2026-02-19 00:18 -- WARNING
1. **[INFO] vk01 GPU thermal cycling confirmed — 9°C swing in 15 minutes during idle** — Temperature went from 18°C to 27°C between snapshots, following the previous 6°C drop. This 15°C peak-to-trough oscillation in 30 minutes during constant zero-utilization load indicates the GPU fan has a wide hysteresis band (fan stops completely when cool, GPU heats passively, fan restarts aggressively). While not immediately damaging, sustained thermal cycling accelerates solder joint fatigue. Consider setting a minimum fan speed via `nvidia-smi` (`nvidia-smi -pl` or persistent fan curve) to keep the GPU at a steady ~25°C rather than oscillating 18-27°C.
2. **[INFO] vk02 `database-server` memory growing during idle — 1,082MB to 1,094MB (+12MB in 15 min)** — PostgreSQL on vk02 is accumulating shared buffer allocations despite zero active connections and no production writes. At 12MB per 15 minutes (~48MB/hour), this could indicate background autovacuum activity on stale tables, or uncommitted WAL segments. Run `SELECT pg_size_pretty(pg_database_size(datname)), datname FROM pg_database;` on vk02 port 2345 to check actual database sizes and `SELECT * FROM pg_stat_activity;` to identify any hidden connections not reflected in the active_connections metric.
3. **[SUGGESTION] vk02 `visionking-visualizer` idle CPU rate (0.48) is 6x higher than vk01's (0.08) — unchanged from previous report** — This disparity persists between snapshots and was previously flagged. The fact that vk02's rate is stable at 0.48 (not growing) suggests a steady-state polling loop rather than a runaway process. The most likely cause is a frontend WebSocket or SSE connection from the vk02 frontend container (which is on the same node) maintaining an active rendering/polling loop, while vk01's visualizer has no local frontend consumer. Verify whether the vk02 frontend's visualizer endpoint is being actively polled.

## 2026-02-19 00:33 -- WARNING
1. **[WARNING] vk01 GPU metrics (memory, temperature) have gone null — potential GPU exporter or driver failure** — Between the 03:15 and 03:30 snapshots, vk01's `gpu_mem_pct` dropped from 67.4% to null and `gpu_temp_c` from 27°C to null, while `gpu_util_pct` remains at 0%. This indicates the `nvidia-gpu-exporter` (DCGM exporter) on vk01 can no longer read GPU telemetry. Check `docker logs nvidia-gpu-exporter` on vk01 for DCGM errors, and run `nvidia-smi` directly on the host to verify the GPU driver is still responsive. If the driver has entered a fault state, the inference container (which is still reporting 5.97GB memory usage via Docker stats) may also be unable to run inference when production resumes. This should be verified before the next shift.
2. **[INFO] vk01 and vk02 database-server error log PIDs are advancing, confirming active probing** — vk01 PIDs jumped from 13168/13175 (03:01 UTC) to 14518/14525 (03:16 UTC), a delta of ~1350 PIDs in 15 minutes. Each probe spawns 2 short-lived PostgreSQL backend processes. This PID advancement rate (~90 PIDs/min) is dominated by the `visionking` database probes. While PostgreSQL handles this gracefully, the PID churn adds unnecessary entries to `pg_stat_activity` history and bloats CSV logs if enabled. Fixing the connection string on vk03 will eliminate this overhead.
3. **[INFO] vk01 cadvisor memory dropped significantly — 195MB to 168MB (-27MB)** — This 14% reduction in cadvisor's memory footprint may indicate cadvisor performed an internal cache eviction or garbage collection. Given that cadvisor is scraping 27 containers on vk01, its 168MB footprint is reasonable but worth watching — cadvisor memory growth on large container counts is a known issue. If it climbs back above 200MB, consider adding `--docker_only=true` and `--disable_metrics=referenced_memory,cpu_topology,resctrl,hugetlb` flags to reduce its collection scope.

## 2026-02-19 00:48 -- WARNING
1. **[WARNING] vk01 GPU telemetry blackout now sustained for 30+ minutes across two snapshot cycles** -- The null readings for `gpu_mem_pct` and `gpu_temp_c` on vk01 have persisted from the previous snapshot (03:30) through the current one (03:45), while `gpu_util_pct` still reports 0%. This is no longer a transient scrape failure -- the DCGM exporter has lost communication with the GPU driver. The Docker-level container stats still show `visionking-inference` at 5.97GB memory, meaning the container process is alive but its GPU access state is unknown. Before the next production shift: (a) check `docker logs nvidia-gpu-exporter` on vk01 for DCGM errors, (b) run `nvidia-smi` on the host to test driver responsiveness, and (c) if the driver is unresponsive, a host reboot may be required to recover the GPU.
2. **[INFO] vk03 Prometheus memory grew 17% in 15 minutes during idle (242MB → 283MB)** -- While vk01 and vk02 Prometheus instances compacted (both decreased), vk03's grew by 41MB. vk03 Prometheus is scraping only 10 containers (vs 27-29 on vk01/vk02), so this growth rate is disproportionate. Check whether vk03's Prometheus has different retention or scrape interval settings than the other nodes. If vk03's `scrape_interval` is shorter (e.g., 5s vs 15s), it would explain the faster ingestion despite fewer targets.
3. **[INFO] vk02 GPU temperature stabilized at 23°C -- no thermal cycling observed** -- Unlike vk01's previously reported 18-27°C oscillation, vk02's GPU temperature moved only 2°C (21→23°C) between snapshots. This differential behavior between identical hardware suggests vk01's fan control issue may be node-specific (BIOS fan curve, physical fan obstruction, or thermal paste degradation on vk01). When vk01's GPU telemetry recovers, compare fan RPM readings between nodes to isolate the cause.
4. **[SUGGESTION] vk02 `is-sis-surface` (image saver) container has accumulated 120MB of writable layer data** -- The container's `Size` field shows `120MB (virtual 295MB)`, up from `120MB (virtual 294MB)` in the previous snapshot. This writable layer growth during idle suggests the image saver is writing temporary files or logs inside the container filesystem rather than to mounted volumes. Over time this bloats the container's diff layer and slows `docker commit`/migration operations. Mount an external volume for any temp file operations, or add a periodic cleanup inside the container.

## 2026-02-22 20:16 -- WARNING
1. **[WARNING] `bar_uuid` and `part_uuid` are systematically 'empty' during active production, causing 100% inspection data loss to PostgreSQL** — Both `dw-sis-surface` (ERROR-level) and `visionking-result-writer` (WARNING-level) are rejecting every message because the bar/part UUID fields contain the literal string `'empty'`. This is happening while bars are actively being inspected (F32331400601-F32331400804). The `visionking-result` service CAN identify bars (it writes pass/fail to Redis by bar ID), but this identification is not reaching the database writer message path. Investigate the message enrichment step between inference output and database writer input — the component responsible for attaching `bar_uuid` to inference results is either misconfigured or the PLC tracking trigger that associates frames to bars is not firing for the database writer queue.
2. **[WARNING] Redis hash `DB_Speed` missing field `Velocidade_Tratada` — inference defaulting to 100% frame sampling** — The inference `batch_processor.py:233` logs that the speed field expected from PLC is absent, forcing it to process every frame rather than subsampling based on line speed. During active production at 2631 mm/min, this means significantly more GPU work than necessary. Verify the `visionking-plc-monitor` is writing speed data to the correct Redis DB and key name. The PLC monitor logs show `writeBar` completing successfully, but the data may be going to a different Redis database or key than what inference expects.
3. **[INFO] vk01 GPU recovered from previous telemetry blackout — now reporting 36°C and 67.4% memory** — The GPU null readings reported in the 00:33-00:48 UTC improvements have resolved. The GPU is now healthy and actively processing inference. The DCGM exporter recovered without intervention, likely after a driver-level transient. No immediate action needed, but this confirms the GPU driver on vk01 has an intermittent communication issue with DCGM worth monitoring.
4. **[INFO] vk01 `database-server` (PostgreSQL) memory grew 300MB in 2.5 hours during production (15.8GB → 16.1GB)** — This is the largest memory consumer on vk01 at 16GB. The growth during active production is expected (shared buffers, work_mem for queries), but at this trajectory it will consume significant host RAM over a full production shift. Monitor for OOM pressure on vk01 given inference (15.1GB) + PostgreSQL (16.1GB) already total 31.2GB.
5. **[SUGGESTION] Inference image loading race condition is pervasive across all 4 cameras** — The `File is empty` warnings appear for all camera IDs (DA3488406, DA3488407, DA3488408, DA3488409) at ~3-4 second intervals. While the 3-retry mechanism handles this, the frequency suggests a systematic timing issue: the image saver notifies inference via RabbitMQ before the `.bin` file is fully flushed to disk. Adding an `fsync` or a brief write-completion signal before publishing to the queue would eliminate these retries and reduce inference latency by ~0.2-0.4s per affected frame.
6. **[INFO] vk02 is fully dark — reachable at network level but running zero containers** — In the previous analysis cycle vk02 had running containers. It now shows empty container lists, null Prometheus host metrics, and persistent ssh_error on health endpoints. Only the RabbitMQ queues on vk02 still show consumers (likely connected from vk01's consumers to vk02's broker). This effectively makes the deployment single-node (vk01) for all processing, with no redundancy for cameras, inference, or database writing.
