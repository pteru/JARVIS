## VisionKing 03002 — Laminacao/Steel — Status Report

**Timestamp:** 2026-02-19 03:00:01 UTC | **Snapshot Delta:** ~17 minutes

---

### SEVERITY: WARNING

### Executive Summary

The system is **idle** — all three nodes are reachable, all containers running, but the production line is not active (0% GPU utilization, empty Redis databases, zero RabbitMQ throughput). The primary ongoing issues remain: the `visionking` database misconfig generating FATAL errors every minute on vk01/vk02, recurring OOM patterns on vk02's visualizer, and vk01's root disk at 81.6% utilization. No new critical failures since the previous snapshot.

---

### Node Status

| Node | CPU | RAM | Disk (root) | Disk (img) | GPU Util | GPU Mem | GPU Temp | Uptime | Containers | Status |
|------|-----|-----|-------------|------------|----------|---------|----------|--------|------------|--------|
| vk01 | 2.9% | 13.1% | **81.6%** | 17% | 0% | 67.4% | 18°C | 2.4h | 27 running | WARNING (disk) |
| vk02 | 4.7% | 14.6% | 16.9% | 41% | 0% | 67.4% | 26°C | 4.8h | 29 running | OK |
| vk03 | 0.5% | 26.1% | 25.6% | N/A | N/A | N/A | N/A | 5.3h | 10 running | OK |

---

### Pipeline Health

| Stage | vk01 | vk02 | Status |
|-------|------|------|--------|
| Cameras (ca-sis-surface) | 4x running, ~0.1 CPU each | 4x running, ~0.11 CPU each | Idle but healthy |
| Redis DB0 (frame cache) | 0 keys | 0 keys | Empty — no frames |
| Image Saver (is-sis-surface) | healthy (all services OK) | healthy (all services OK) | OK |
| RabbitMQ queues | 3 queues, 0 msgs, 0 rate | 3 queues, 0 msgs, 0 rate | Idle |
| Inference (GPU/ONNX) | ~0 CPU, 5.97GB mem | ~0 CPU, 5.97GB mem | Models loaded, idle |
| DB Writer (dw-sis-surface) | ~0 CPU | ~0 CPU | Idle |
| Result Writer | ~0 CPU | N/A | Idle |
| PostgreSQL | 0 connections, 0 bytes | 0 connections, 0 bytes | Idle |
| Backend API | N/A (on vk02/vk03) | ~0 CPU | Idle |
| Frontend | N/A (on vk02/vk03) | 0 CPU | Serving |

**Pipeline verdict:** Fully idle. No frames flowing, no inference running, no queue backlog. All components are up and ready to process when the line starts.

---

### PLC & Production State

| Parameter | vk01 | vk02 |
|-----------|------|------|
| Redis DB1 keys | 0 | 0 |
| PLC values | empty | empty |

**The production line is NOT running.** Redis DB1 contains no PLC state — no speed, no bar tracking, no triggers. The PLC monitors (`visionking-plc-monitor` on vk01, `pm-sis-surface` on vk02) are executing their read/write cycles every ~30-70ms but reading empty/zero values from the PLC, confirming the mill is offline.

---

### GUI & Infrastructure Status

| Endpoint | Status | Notes |
|----------|--------|-------|
| vk01 Frontend | 200 | OK |
| vk01 Portainer | 200 | OK |
| vk01 RedisInsight | 200 | OK |
| vk01 RabbitMQ UI | 200 | OK |
| vk01 pgAdmin | 302 | Redirect to login — normal |
| vk01 Grafana | 302 | Redirect to login — normal |
| vk02 Frontend | 200 | OK |
| vk02 Portainer | 200 | OK |
| vk02 RedisInsight | 200 | OK |
| vk02 RabbitMQ UI | 200 | OK |
| vk02 pgAdmin | 302 | Normal |
| vk02 Grafana | 302 | Normal |
| vk03 Frontend | 200 | OK |
| vk03 Portainer | 200 | OK |
| vk03 Grafana | 302 | Normal |

All web interfaces are responding normally. No changes from previous snapshot.

---

### Changes Since Last Check (17-minute delta)

**Metrics:**
- vk01 GPU temp dropped 24°C → 18°C (6°C drop in 17 min — likely thermal management in idle, or ambient cooling)
- vk01 uptime increased 7626s → 8646s (+1020s ≈ 17 min, confirming no reboot)
- vk02 uptime increased 16370s → 17390s (+1020s, confirming no reboot)
- vk03 uptime increased 18132s → 19152s (+1020s, confirming no reboot)
- vk01 Prometheus memory: 768MB → 789MB (+21MB, ingesting)
- vk02 Prometheus memory: 655MB → 690MB (+35MB, ingesting)
- vk03 Prometheus memory: 260MB → 255MB (-5MB, stable)

**Log changes:**
- vk01 `database-server` FATAL errors **disappeared** from current snapshot error logs (were present in previous). This could mean: (a) the log window shifted past them, or (b) the probing client stopped temporarily.
- vk02 `dw-sis-surface` RabbitMQ reconnect error from Feb 18 18:48 still present in log history.
- vk02 `visionking-visualizer` added new worker crash entries from Feb 18 19:08 (same as previous snapshot — no new crashes).
- PLC monitor logs on both nodes show continued normal cycling with timestamps advancing (vk01: 00:01:53, vk02: 23:38:43-44).

**Container changes:**
- No container restarts, no new containers, no removed containers across any node.

---

### Trending Concerns (24h)

1. **vk01 root disk at 81.6% — flat but dangerous.** All 14 samples show 81% consistently. No growth, but no headroom either. One production shift of image saving could push this critical.

2. **GPU memory locked at 67.4% across all samples on both nodes.** Models remain loaded consuming ~6GB VRAM each despite 24h of zero utilization. Thermal wear on vk01 is minimal (18°C) but vk02 at 26°C suggests slightly worse cooling.

3. **vk01 Prometheus memory trending up this snapshot cycle.** 768MB → 789MB in 17 minutes = ~1.2MB/min. Below the 13.4MB/min rate seen earlier, suggesting the ingestion rate slows during sustained idle. Still, at this rate it will reach ~850MB before the next compaction cycle.

4. **vk02 CPU consistently higher than vk01** (4.7% vs 2.9%). The delta is accounted for by the `visionking-visualizer` on vk02 consuming 0.49 CPU (vs 0.08 on vk01), likely due to its more active worker recycling behavior.

---

### Error Log Analysis

| Service | Node | Pattern | Severity | Status |
|---------|------|---------|----------|--------|
| database-server | vk01/vk02 | `FATAL: database "visionking" does not exist` | WARNING | Ongoing, ~1/min |
| visionking-visualizer | vk02 | Worker OOM SIGKILL + exit code 1 | WARNING | Historical (Feb 4, 10, 18) |
| dw-sis-surface | vk02 | PostgreSQL unreachable at 10.244.70.50:2345 | INFO | Historical (Feb 4, 21) |
| dw-sis-surface | vk02 | RabbitMQ reconnection failures | INFO | Historical, recovered |
| ct-sis-surface | vk02 | `sw::redis::IoError` + "Falha ao setar IP1" | INFO | Historical (Feb 4, 18) |
| sm-sis-surface | vk01/vk02 | Docker daemon log grab error | INFO | Benign — large log buffer issue |

**Cross-correlation:** The `ct-sis-surface` Redis IoError and `dw-sis-surface` RabbitMQ/PostgreSQL errors on vk02 from Feb 18 18:48 occurred at the exact same timestamp, confirming a brief network disruption or service restart event on vk02 around that time. All services recovered.

---

### Recommendations

**Immediate (fix now):**
- None critical. System is idle and stable.

**Short-term (this week):**
- Fix the `visionking` database name in the backend connection strings (vk03 backends probing vk01/vk02 PostgreSQL)
- Add `--max-requests 1000 --max-requests-jitter 50` to vk02 visualizer gunicorn before next production shift
- Clear space on vk01 root disk or move image storage to a dedicated volume

**Long-term (architectural):**
- Standardize inference versions across vk01 (dev c014) and vk02 (prod c009)
- Mirror critical vk02-only services (`visionking-length-measure`, `rc-sis-surface`) for redundancy
- Implement container health checks and auto-restart policies for OOM-prone services

---

### NEW IMPROVEMENTS

1. **[INFO] vk01 GPU temperature dropped 6°C in 17 minutes (24°C → 18°C) during idle** — This unusually rapid cooling suggests either the GPU fan profile is aggressive in idle mode, or ambient temperature changed (night cooling). While not problematic, a 6°C swing in 17 minutes during constant idle load is worth noting. If temperature oscillates this much, it may cause thermal cycling stress on the GPU. Monitor for GPU throttling events when production resumes to verify thermal management is adequate under load.

2. **[INFO] vk01 `database-server` FATAL errors disappeared from current error log window** — The `database "visionking" does not exist` errors that appeared in the previous snapshot's error logs for vk01 are absent from the current snapshot, while vk02's are still present. This likely means vk01's error log window rotated past those entries (they were from 02:30-02:35 UTC, now 17 minutes later). However, it could also indicate the probing client targeting vk01 stopped while vk02's continues — worth verifying whether the vk03 backend connects to both nodes or only vk02.

3. **[SUGGESTION] vk02 `visionking-visualizer` idle CPU consumption is disproportionately high** — At 0.49 CPU rate during complete idle (no frames, no users), this container is consuming 6x more CPU than vk01's identical visualizer (0.08 CPU). This 6x differential with no production load suggests a busy-loop, excessive polling, or a stuck worker in the vk02 instance. Investigate with `top` inside the container or check for a websocket reconnection loop that may be spinning without backoff.
