

## VisionKing 03002 — Health Analysis Report

### SEVERITY: WARNING

### Executive Summary

All three nodes (vk01, vk02, vk03) are **unreachable via SSH**, meaning no system-level metrics (CPU, RAM, Disk, GPU, containers, logs) could be collected. However, most web GUIs are responding normally (HTTP 200/302), indicating the nodes are online and Docker services are running — the SSH connectivity issue is isolated to the monitoring layer, not a full outage. Three GUI endpoints are down (vk03 pgAdmin, vk01/vk02 visualizer).

### Node Status

| Node | CPU | RAM | Disk | GPU | Containers | Status |
|------|-----|-----|------|-----|------------|--------|
| vk01 | N/A | N/A | N/A | N/A | N/A | SSH unreachable, GUIs responding |
| vk02 | N/A | N/A | N/A | N/A | N/A | SSH unreachable, GUIs responding |
| vk03 | N/A | N/A | N/A | N/A | N/A | SSH unreachable, GUIs responding |

No system-level metrics are available due to SSH being unreachable on all nodes. This is the primary gap in this analysis.

### Pipeline Health

**Unable to assess.** Pipeline health requires container logs, RabbitMQ queue depths, Redis state, and GPU utilization — all of which depend on SSH or API access that was unavailable. The fact that frontends are responding on vk01 and vk02 suggests the backend and database layers are functional, but we cannot confirm whether:
- Cameras are acquiring frames
- Inference is processing at expected throughput
- Queue depths are nominal or building up
- Database writers are keeping pace

### PLC & Production State

**Unable to assess.** PLC state is stored in Redis DB1 and requires direct access to read speed, material position, part presence, and trigger data. Without SSH, we cannot determine if the production line is running.

### GUI & Infrastructure Status

| Endpoint | Status | Assessment |
|----------|--------|------------|
| vk01 Frontend | 200 | OK |
| vk01 Portainer | 200 | OK |
| vk01 Redis Insight | 200 | OK |
| vk01 RabbitMQ UI | 200 | OK |
| vk01 pgAdmin | 302 | OK (redirect to login) |
| vk01 Grafana | 302 | OK (redirect to login) |
| vk02 Frontend | 200 | OK |
| vk02 Portainer | 200 | OK |
| vk02 Redis Insight | 200 | OK |
| vk02 RabbitMQ UI | 200 | OK |
| vk02 pgAdmin | 302 | OK (redirect to login) |
| vk02 Grafana | 302 | OK (redirect to login) |
| vk03 Frontend | 200 | OK |
| vk03 Portainer | 200 | OK |
| vk03 pgAdmin | **0** | **DOWN** |
| vk03 Grafana | 302 | OK (redirect to login) |
| vk01 Visualizer | **0** | **DOWN** |
| vk02 Visualizer | **0** | **DOWN** |

**3 endpoints are down:**
- **vk03 pgAdmin (status 0):** The database administration UI on the dashboard node is unreachable. This could indicate the pgAdmin container has crashed or failed to start. Since vk03 is the PostgreSQL host, this warrants investigation — though PostgreSQL itself may still be running (the frontends depend on it and are responding).
- **vk01/vk02 Visualizer (status 0):** Both visualizer services are down. This is the real-time frame visualization tool. It may not be deployed in this configuration, or the containers may have stopped.

### Changes Since Last Check

No previous snapshot is available — this is the first collected data point. No delta analysis is possible.

### Trending Concerns

Insufficient data for trend analysis (only 1 snapshot today). Once multiple snapshots are collected, watch for:
- Disk usage growth rate (vk01 was previously flagged at 81% in earlier sessions)
- Container restart count increases
- Queue depth trends

### Error Log Analysis

**Unable to perform.** Error log analysis requires SSH access to read container logs via `docker logs`. Without SSH connectivity, no cross-service error correlation is possible.

### Recommendations

**Immediate:**
1. **Restore SSH access to all three nodes.** This is the top priority — without SSH, the health monitoring system is effectively blind. Investigate whether SSH keys have expired, firewall rules changed, or the SSH daemon has crashed. Try accessing via Portainer's web terminal (which is responding on all nodes) as a workaround.
2. **Investigate vk03 pgAdmin (status 0).** Use vk03 Portainer (which is up) to check the pgAdmin container status and logs.

**Short-term:**
3. **Investigate vk01/vk02 visualizer services.** Determine if these are expected to be down (not deployed) or represent a service failure. If not needed, remove them from monitoring to reduce noise.
4. **Collect a second snapshot** to establish baseline trends and enable delta analysis.

**Long-term:**
5. **Add HTTP-based health probes as a fallback.** Since GUIs responded even when SSH was down, consider adding lightweight `/health` endpoints to critical services (inference, database-writer, image-saver) that can be probed via HTTP without SSH.
6. **Set up Grafana alerting** on the nodes where it's already deployed (all three have Grafana responding) to provide monitoring independent of the JARVIS SSH-based collection.

### NEW IMPROVEMENTS

1. **[WARNING] SSH unreachable on all 3 nodes** — Investigate SSH daemon status, firewall rules, and key/credential validity. Use Portainer web consoles (all responding) as an alternative entry point to diagnose. Consider adding a secondary access method (e.g., WireGuard VPN or Tailscale) for redundancy.
2. **[WARNING] vk03 pgAdmin is down (status 0)** — Check the pgAdmin container via vk03 Portainer. If PostgreSQL itself is also down, the entire dashboard stack is at risk (though frontend responding at 200 suggests the DB is likely fine).
3. **[INFO] vk01/vk02 Visualizer services are down (status 0)** — Clarify whether these are intentionally not deployed in 03002 or represent failed containers. If not needed, remove from the monitored endpoints list to avoid false warnings.
4. **[SUGGESTION] Implement HTTP health endpoints as SSH-independent monitoring** — The current monitoring is entirely SSH-dependent. Adding `/health` or `/ready` endpoints to key pipeline services would allow health checks even when SSH is unavailable, providing a more resilient observability layer.
