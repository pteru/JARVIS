## VisionKing 03002 — Laminacao/Steel — Diagnostic Report

**Timestamp:** 2026-02-19T02:15:01Z | **Snapshot delta:** ~6.5 minutes

---

### SEVERITY: WARNING

---

### Executive Summary

The system is **idle** — no production activity detected. All 3 nodes are reachable, all containers are running, and infrastructure services are healthy. The persistent critical issues remain unchanged: the `visionking` database does not exist on vk01/vk02 PostgreSQL instances (FATAL errors recurring every ~10 minutes), and all Redis databases are completely empty (0 keys across all DBs on both processing nodes), confirming the line is not running. Disk pressure on vk01 remains the most concerning trend at 81.6%.

---

### Node Status

| Node | CPU | RAM | Disk | GPU Util | GPU Mem | GPU Temp | Containers | Uptime | Status |
|------|-----|-----|------|----------|---------|----------|------------|--------|--------|
| vk01 | 3.1% | 13.1% | **81.6%** | 0% | 67.4% | 26°C | 27 running | 1.7h | WARNING (disk) |
| vk02 | 4.6% | 14.4% | 16.9% | 0% | 67.4% | 25°C | 29 running | 4.1h | OK |
| vk03 | 0.5% | 25.9% | 25.6% | N/A | N/A | N/A | 10 running | 4.6h | OK |

**Notes:**
- vk01 uptime is only 1.7h (~5946s) vs vk02 at 4.1h — vk01 rebooted more recently than vk02
- GPU models loaded on both nodes (67.4% VRAM) but 0% utilization — idle inference
- vk03 RAM at 25.9% is the highest proportionally but stable

---

### Pipeline Health

**Pipeline is IDLE.** Every stage shows zero throughput:

| Stage | Indicator | Value | Status |
|-------|-----------|-------|--------|
| Cameras | Redis DB0 keys (vk01/vk02) | 0 / 0 | No frames cached |
| Redis DB2 (camera) | Keys | 0 / 0 | No camera data |
| Image Saver | Health endpoint | Healthy (both) | Service up, no work |
| RabbitMQ queues | Messages | 0 across all 6 queues | Empty |
| RabbitMQ | Publish/deliver rate | 0 / 0 | Zero throughput |
| Inference | CPU rate | 0.0002 / 0.0003 | Idle |
| DB Writers | CPU rate | ~0.00002 | Idle |
| PostgreSQL | Active connections | 0 / 0 (vk01/vk02) | No writes |

All 6 RabbitMQ queues (3 per node) have 1 consumer each, 0 messages — services are connected and waiting but no data is flowing.

---

### PLC & Production State

**Line is NOT running.** Redis DB1 on both nodes contains **zero keys** — no PLC tags are being stored. This means:

- No speed data, no material position, no part presence
- The PLC monitor containers (`visionking-plc-monitor` on vk01, `pm-sis-surface` on vk02) are actively cycling through their read/write loops (visible in WARNING logs) but either the PLC is not connected or returning no meaningful data
- The PLC cycle is completing normally (~50ms per iteration) suggesting the PLC connection itself is alive, but the line is stopped

This is consistent with a 2:15 AM local time snapshot — the steel lamination line is likely shut down for the night.

---

### GUI & Infrastructure Status

| Endpoint | Status | Notes |
|----------|--------|-------|
| vk01 frontend | 200 | OK |
| vk01 portainer | 200 | OK |
| vk01 redis insight | 200 | OK |
| vk01 rabbitmq ui | 200 | OK |
| vk01 pgadmin | 302 | Redirect (normal — login page) |
| vk01 grafana | 302 | Redirect (normal — login page) |
| vk02 frontend | 200 | OK |
| vk02 portainer | 200 | OK |
| vk02 redis insight | 200 | OK |
| vk02 rabbitmq ui | 200 | OK |
| vk02 pgadmin | 302 | Redirect (normal) |
| vk02 grafana | 302 | Redirect (normal) |
| vk03 frontend | 200 | OK |
| vk03 portainer | 200 | OK |
| vk03 grafana | 302 | Redirect (normal) |

All web interfaces responding normally. The 302s are expected authentication redirects.

---

### Changes Since Last Check (~6.5 minutes delta)

**Minimal changes — system is stable in idle state:**

- **vk01 uptime:** 5556s → 5946s (+390s, normal increment)
- **vk01 CPU:** 3.0% → 3.1% (negligible)
- **vk01 RAM:** 13.2% → 13.1% (slight decrease, ~100MB freed)
- **vk01 Prometheus memory:** 787MB → 694MB (dropped ~93MB — likely a metric retention compaction)
- **vk02 CPU:** 4.8% → 4.6% (slight decrease)
- **vk02 RAM:** 14.8% → 14.4% (slight decrease, ~330MB freed)
- **vk02 Prometheus memory:** 460MB → 541MB (grew ~81MB — normal scrape accumulation)
- **vk02 GPU temp:** 27°C → 25°C (cooling, consistent with idle)
- **vk02 `visionking-visualizer`:** CPU rate 0.504 → 0.483 (minor decrease)
- **vk01 `visionking-visualizer`:** CPU rate 0.096 → 0.086 (minor decrease)
- **PostgreSQL "visionking" FATAL errors:** Continued on both nodes (vk01: 02:01 → 02:10, vk02: 01:38 → 01:47) — recurring every ~9 minutes
- **vk03:** Essentially unchanged across all metrics

**No container restarts, no new error patterns, no queue buildup.**

---

### Trending Concerns (24h)

| Metric | Node | Trend | Concern |
|--------|------|-------|---------|
| Disk | vk01 | Flat at 81% | **HIGH** — stable but dangerously high. One large log rotation or image batch could push into critical territory |
| GPU Util | vk01/vk02 | Flat at 0% | Expected during night shift; verify it activates during production |
| GPU Mem | vk01/vk02 | Flat at 67.4% | Models loaded, consistent — good |
| CPU | vk01 | Range 2-7%, avg 4.4 | The spike to 7% likely correlates with the recent reboot |
| RAM | vk03 | 25-26%, avg 25.6 | Stable, but highest of the three nodes proportionally |

No alarming directional trends. The main concern remains vk01 disk at 81.6% which has been flat but leaves no safety margin.

---

### Error Log Analysis

**1. PostgreSQL "visionking" database FATAL (vk01 + vk02) — PERSISTENT**
```
FATAL: database "visionking" does not exist
```
- Recurring every ~9 minutes on both nodes
- Source is an unknown client connecting with the wrong database name
- Not causing service disruption but polluting logs and wasting connection slots

**2. vk02 `visionking-visualizer` worker crashes — HISTORICAL**
- Latest crash: 2026-02-18 19:08:06 (workers pid:7 and pid:10 exited code 1)
- Previous crashes: Feb 4 (SIGKILL/OOM), Feb 10 (cascade crash with logging RuntimeError)
- Currently running stable at 315MB with 0.48 CPU rate
- Pattern: memory accumulation → OOM kill → respawn cycle

**3. vk02 `dw-sis-surface` — HISTORICAL network issues**
- PostgreSQL connection failures to `10.244.70.50:2345` (Network unreachable) — from Feb 4 reboot window
- RabbitMQ reconnection events on Feb 17 and Feb 18
- Currently idle but connected (consumer registered on queue)

**4. vk02 `ct-sis-surface` Redis IoError — HISTORICAL**
- `sw::redis::IoError` crashes and "Falha ao setar IP1!" errors
- Correlated with reboot events (Feb 4, Feb 18)
- Currently running stable

**5. PLC monitors — NORMAL but verbose**
- Both `visionking-plc-monitor` (vk01) and `pm-sis-surface` (vk02) logging full cycle traces at WARNING level
- This is operational noise, not errors

**No cascade failures detected. No new error patterns since last check.**

---

### Recommendations

**Immediate (fix now):**
- None required — system is in stable idle state during night shift

**Short-term (this week):**
1. **Free disk space on vk01** — At 81.6%, any production restart with image saving could fill the disk. Clean Docker images, old logs, and unused data volumes
2. **Fix the "visionking" database connection string** — Identify which service is connecting with the wrong DB name and update its configuration. This has been recurring for at least the entire monitoring window
3. **Restart vk03 backend** if the Redis reconnection issue from the previous report is still present (not directly visible in this snapshot's error logs, but was flagged earlier)

**Long-term (architectural):**
1. Implement `gunicorn --max-requests` on vk02 visualizer to prevent OOM accumulation
2. Reclassify PLC monitor log levels from WARNING to DEBUG
3. Add automated disk space alerts at 75% and 85% thresholds

---

### NEW IMPROVEMENTS

1. **[INFO] vk01 Prometheus memory dropped 93MB between snapshots (787MB → 694MB)** — This is likely a TSDB head compaction or block retention event. While normal, Prometheus on vk01 at ~694MB is the single largest non-inference container. If vk01 disk pressure worsens, consider reducing `--storage.tsdb.retention.time` or moving Prometheus to vk02 which has 83% disk headroom.

2. **[INFO] Version inconsistency between vk01 and vk02 inference containers** — vk01 runs `dev-visionking-inference:c014` (dev tag, newer commit `a8399db`) while vk02 runs `visionking-inference:c009` (prod tag, older commit `3a0eb21`). This means the two processing nodes are running different inference model versions. If this is intentional A/B testing, document it. If not, align both nodes to the same version before the next production shift to ensure consistent defect detection.

3. **[SUGGESTION] vk02 has additional pipeline services not present on vk01** — vk02 runs `visionking-length-measure`, `rc-sis-surface`, and `pm-sis-surface` that vk01 does not have (vk01 has `visionking-plc-monitor` and `dw-sis-surface` instead of `pm-sis-surface` and `rc-sis-surface`). This asymmetric deployment means the two nodes are not interchangeable — if vk02 goes down, length measurement and the `rc` pipeline stage are lost with no redundancy. Document the asymmetry and evaluate whether critical services should be mirrored.

4. **[INFO] All Redis databases empty across both nodes (DB0-DB3)** — This is expected during line downtime, but confirms there is no persistent state surviving across production sessions in Redis. If the line starts unexpectedly, all camera configs (DB2), PLC state (DB1), and settings (DB3) will need to be re-populated. Verify that the initialization sequence handles cold-start from empty Redis gracefully.
