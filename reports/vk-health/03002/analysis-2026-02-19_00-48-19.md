

## VisionKing 03002 — System Analysis Report

### SEVERITY: WARNING

### Executive Summary

The system is in **inter-bar idle state** following recent production activity. All queues have drained to zero and pipeline throughput has stopped, but production was active within the last hour (bars O46401002501-O46401002701 processed between 07:46-08:01 UTC). The persistent `bar_uuid='empty'` data loss bug and `is-sis-surface` memory leak continue unchecked from previous reports, and vk02 remains fully dark with zero containers running.

### Node Status

| Node | CPU | RAM | Disk (root) | Disk (img) | GPU Util | GPU Mem | GPU Temp | Containers | Status |
|------|-----|-----|-------------|------------|----------|---------|----------|------------|--------|
| vk01 | 5.5% | 19.1% | 81.8% | 25% | 0% | 67.4% | 30°C | 27 running | Operational (idle) |
| vk02 | null | null | null | 44% | null | null | null | 0 running | Dark/Offline |
| vk03 | 0.5% | 25.2% | 25.7% | n/a | n/a | n/a | 277k uptime | 10 running | Healthy |

### Pipeline Health

**Current state: Idle (inter-bar gap or line pause)**

- **RabbitMQ queues (vk01):** All 3 queues at 0 messages, 0 publish/deliver rate. In the previous snapshot, queues were actively flowing at ~54-66 msg/s. Production has stopped between the two snapshots.
- **Redis DB0 (frame cache):** 0 keys on both nodes — no frames in transit.
- **Image Saver:** Reporting healthy (memory, rabbitmq, redis, storage all true).
- **Inference:** Running but idle. GPU at 0% utilization, 67.4% memory (models loaded).
- **Database Writers:** Both `dw-sis-surface` and `visionking-result-writer` are running but continuing to produce errors during production windows.

**Error cascade during production (07:46-08:01 UTC):**
1. Inference loads images → many `.bin` files are empty on first attempt (race condition with image saver) → retries succeed
2. Inference produces results with `part_uuid=empty` → `visionking-result-writer` discards entire batches as invalid UUIDs
3. Same messages reach `dw-sis-surface` → `ValueError: UUID 'empty' for field 'bar_uuid'` → batch processing errors
4. `visionking-result` successfully writes pass/fail to Redis by bar ID (bars O46401002501-O46401002701 all wrote `false`)

**Net result:** Inference runs, results are computed, but **zero inspection data reaches PostgreSQL**. The pass/fail flag reaches Redis only.

### PLC & Production State

- **Redis DB1 (PLC):** Empty keys/values — PLC state is not being cached or has been flushed.
- **Line status:** Was recently active. The `visionking-result` position monitor shows position **170,521mm** (exceeding the 70,000mm threshold), triggering the `BOOL_SPARE_13_STATUS = true` reset cycle. Position advanced from 155,997mm (previous snapshot at 05:40) to 170,521mm (current at 07:54), confirming the line moved ~14.5 meters between snapshots.
- **Speed:** Unknown — `Velocidade_Tratada` field is missing from `DB_Speed` Redis hash. Inference defaults to 100% sampling.
- **Bar tracking:** Bars O46401002501 through O46401002701 were processed. The numbering pattern (25xx, 26xx, 27xx series) suggests sequential bar production on order O464010.
- **PLC monitor cycle:** Operating normally at ~25ms per full cycle (readBar→redisRead→setBar→writeBar), ~20 cycles/second.

### GUI & Infrastructure Status

| Endpoint | Status | Note |
|----------|--------|------|
| vk01 Frontend | 200 | OK |
| vk01 Portainer | 200 | OK |
| vk01 Redis Insight | 200 | OK |
| vk01 RabbitMQ UI | 200 | OK |
| vk01 PgAdmin | 302 | Redirect (normal — login page) |
| vk01 Grafana | 302 | Redirect (normal — login page) |
| vk02 Frontend | 200 | OK (but no containers running — serving cached/static page?) |
| vk02 Portainer | 200 | OK |
| vk02 Redis Insight | 200 | OK |
| vk02 RabbitMQ UI | 200 | OK |
| vk02 PgAdmin | 302 | Redirect |
| vk02 Grafana | 302 | Redirect |
| vk03 Frontend | 200 | OK |
| vk03 Portainer | 200 | OK |
| vk03 Grafana | 302 | Redirect |

All GUIs responding. vk02 endpoints returning 200 despite zero containers is notable — these may be host-level services or stale responses.

### Changes Since Last Check

| Metric | Previous (08:45 UTC) | Current (11:00 UTC) | Delta |
|--------|----------------------|----------------------|-------|
| vk01 CPU | 13.6% | 5.5% | -8.1% (production ended) |
| vk01 RAM | 19.2% | 19.1% | Stable |
| vk01 uptime | 31,826s (8.8h) | 39,911s (11.1h) | +2.25h elapsed |
| vk01 disk root | 81.8% | 81.8% | Stable |
| RabbitMQ publish rate | 53-66 msg/s | 0 msg/s | Production stopped |
| vk01 `database-server` mem | 6.7GB | 8.8GB | +2.1GB in 2.25h |
| vk01 `is-sis-surface` mem | 10.7GB | 10.7GB | Stable (was growing in previous cycles) |
| vk01 `visionking-inference` mem | 9.1GB | 8.9GB | -200MB (slight decrease) |
| vk01 `visionking-visualizer` mem | 359MB | 392MB | +33MB in 2.25h |
| vk01 Prometheus mem | 850MB | 628MB | -222MB (TSDB compaction) |
| vk03 uptime | 269,609s (3.1d) | 277,709s (3.2d) | Normal |
| vk03 Prometheus mem | 497MB | 519MB | +22MB |
| vk03 `database-server` mem | 139MB | 139MB | Stable |
| GPU temp | 30°C | 30°C | Stable |

**Key changes:**
- Production shifted from active (bars being inspected at 53-66 msg/s) to idle (0 msg/s)
- CPU dropped from 13.6% to 5.5% reflecting end of inference workload
- PostgreSQL on vk01 grew another 2.1GB during/after production (now 8.8GB, up from 1.2GB post-reboot)
- Visualizer grew 33MB in 2.25h (~14.7MB/h — accelerating from previously measured 4MB/h, likely due to production activity)
- `dw-sis-surface` delivery tags jumped from ~243k to ~380k — approximately **137,000 more messages consumed and discarded** during this production window

### Trending Concerns

1. **vk01 disk at 81.8% — flat but dangerously high.** No growth in 24h, but any increase in image retention or PostgreSQL WAL growth could tip it past 85%.
2. **vk01 PostgreSQL memory trajectory:** 1.2GB (post-reboot) → 5.2GB (2h idle) → 6.7GB (production) → 8.8GB (post-production). At ~1GB/hour even during idle, will reach 16GB again within 7 hours.
3. **vk01 `is-sis-surface` stabilized at 10.7GB** — the leak may have plateaued, or the allocation pattern changed. Still consuming 10.7GB for a service that should use <500MB.
4. **vk01 `visionking-visualizer` at 392MB and accelerating** — approaching the ~400MB OOM threshold observed on vk02.
5. **vk02 remains fully offline** — single-node processing with no redundancy continues.

### Error Log Analysis

**Cross-service correlation during production (07:46-08:01 UTC):**

```
Camera → Image Saver → [files written to disk, some with race condition]
                     → RabbitMQ → Inference → [loads files, retries on empty .bin]
                                             → Results with part_uuid='empty'
                                             → visionking-result-writer: DISCARDS (invalid UUID)
                                             → dw-sis-surface: ERROR (ValueError on bar_uuid)
                     Meanwhile:
                     → visionking-result: SUCCEEDS writing pass/fail to Redis by bar ID
```

The root cause chain:
1. **Missing `bar_uuid` enrichment** remains the primary data loss vector. The component between inference output and database writer input still fails to attach bar identity to results.
2. **Missing `Velocidade_Tratada`** in Redis — inference over-samples at 100% instead of speed-adjusted rate, wasting GPU cycles.
3. **Image file race condition** — .bin files read before fully flushed. 100% of failures are Attempt 1/3 and succeed on retry, so data is not lost but latency is added.

**Error volume this window:** ~137,000 messages discarded by database writers (delivery tag delta 243k→380k on dw-sis-surface).

### Recommendations

**Immediate (fix now):**
- Restart `visionking-visualizer` on vk01 before it OOMs at ~400MB (currently 392MB)
- Restart `is-sis-surface` on vk01 to reclaim 10.7GB of leaked memory, add `mem_limit: 4g` to its compose file

**Short-term (this week):**
- Fix the `bar_uuid='empty'` enrichment bug — this is now the longest-standing production data loss issue, affecting every production window since it was first observed
- Configure `Velocidade_Tratada` in the correct Redis hash/DB so inference can subsample frames by line speed
- Set PostgreSQL `shared_buffers = 2GB` and `effective_cache_size = 4GB` on vk01 to cap idle memory growth
- Deploy `--max-requests 1000 --max-requests-jitter 50` on both visualizer containers
- Investigate and restore vk02 to operational status

**Long-term (architectural):**
- Add dead-letter queue for discarded database writer messages to enable retroactive data recovery
- Add `fsync` + write-completion signal before publishing frame availability to RabbitMQ
- Implement container memory limits across all pipeline services on vk01
- Establish vk02 as a hot standby rather than a dark node to provide processing redundancy

### NEW IMPROVEMENTS

1. **[INFO] `visionking-visualizer` on vk01 memory growth rate tripled during production — 4MB/h idle to ~14.7MB/h active (359MB→392MB in 2.25h)** — This confirms the leak has both idle and active components. The idle leak (previously measured at 4MB/h) accelerates 3.6x during production. At 392MB it is now within 2% of the ~400MB crash threshold. A preemptive restart is needed before the next production window, and the `--max-requests` gunicorn fix should be deployed simultaneously.

2. **[INFO] vk01 `database-server` PostgreSQL memory growth rate is consistent at ~1GB/hour regardless of production state** — Growth was 2.1GB over 2.25 hours (spanning both active production and idle), matching the previously observed 1GB/hour idle rate from the 05:44 report. This confirms the memory growth is NOT driven by production queries but by a background process — most likely autovacuum or the periodic connection/disconnection from the vk03 backend's `database "visionking"` probes causing memory fragmentation in PostgreSQL's per-backend allocators.

3. **[INFO] `visionking-result` position threshold behavior is functioning correctly — position advanced 14.5m between production windows** — The position monitor correctly detected position 170,521mm > 70,000mm threshold and set `BOOL_SPARE_13_STATUS = true` for reset. The position delta from the previous snapshot (155,997mm) confirms ~14.5 meters of steel bar movement. The "Valor 'false' escrito no Redis" entries confirm bar-level pass/fail results ARE reaching Redis even though they fail to reach PostgreSQL.

4. **[WARNING] `is-sis-surface` memory stabilized at 10.7GB but may indicate a plateau rather than a fix — verify with production restart** — The image saver was at 10.7GB in the previous snapshot (05:44 UTC, during growth at 2.9GB/h) and is still at 10.7GB now (11:00 UTC, 5.3 hours later). This could mean: (a) the leak reached a natural ceiling (e.g., bounded by available buffers), (b) the leak stopped when production paused, or (c) the measurement is at the same point in a sawtooth pattern. When production resumes, monitor whether it jumps above 10.7GB — if so, the previous 2.9GB/h growth rate may resume and push it toward OOM.

5. **[INFO] RabbitMQ delivery tag delta quantifies production window data loss — ~137,000 inspection results discarded in ~75 minutes of production** — Delivery tags on `dw-sis-surface` advanced from ~243,000 to ~380,000 between snapshots. At 4 cameras producing ~15 frames/second each and ~4 crops per frame, this aligns with approximately 75 minutes of active production. Every single one of these results was computed by GPU inference and then thrown away due to the `bar_uuid='empty'` bug. The cumulative data loss across all reported production windows now exceeds 250,000 inspection results.

6. **[SUGGESTION] vk01 `cache` (KeyDB/Redis) CPU rate decreased 10% between snapshots (0.31→0.28) despite production occurring — investigate whether Redis is being underutilized or bypassed** — Redis CPU should spike during production (frame caching, PLC state reads) and drop during idle. The decrease from 0.31 to 0.28 despite an active production window suggests either: (a) the measurement timing caught the tail of idle→idle transition, or (b) fewer Redis operations are occurring per production cycle than expected. Given the empty `DB_Speed` hash and empty DB1 PLC values, some Redis interactions that should be happening during production may not be.
