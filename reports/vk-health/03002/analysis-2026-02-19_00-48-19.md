## VisionKing 03002 -- Diagnostic Report

### SEVERITY: WARNING

### Executive Summary

The system is idle with all three nodes reachable and all containers running. The pipeline is fully stopped (zero Redis keys, zero queue messages, zero GPU utilization). The primary concern is the persistent vk01 GPU telemetry blackout (null metrics for memory and temperature) which has now continued for at least 30 minutes across two snapshot cycles -- this must be verified before the next production shift. Resource utilization is stable and low across all nodes.

### Node Status

| Node | CPU | RAM | Disk (root) | Disk (img) | GPU Util | GPU Mem | GPU Temp | Containers | Status |
|------|-----|-----|-------------|------------|----------|---------|----------|------------|--------|
| vk01 | 2.9% | 12.9% | **81.6%** | 17% | 0% | **null** | **null** | 27 running | WARNING |
| vk02 | 4.7% | 14.4% | 16.9% | 41% | 0% | 67.4% | 23°C | 29 running | OK |
| vk03 | 0.5% | 26.3% | 25.6% | n/a | n/a | n/a | n/a | 10 running | OK |

### Pipeline Health

**Pipeline is fully stopped.** All indicators confirm zero production activity:

- **Redis**: All databases empty across both nodes (DB0=0 keys, DB1=no keys, DB2=0 keys, DB3=no keys)
- **RabbitMQ**: All 6 queues (3 per node) at zero messages, zero publish/deliver rate. Each queue has exactly 1 consumer connected -- consumers are healthy and waiting.
- **Inference**: Both containers running but GPU utilization at 0%. Models remain loaded in VRAM (~5.97GB per node).
- **Image Saver**: Health endpoints report healthy on both nodes (memory, rabbitmq, redis, storage all true).
- **Database Writers**: Near-zero CPU (`dw-sis-surface` at ~0.00002 CPU rate on both nodes) -- idle but alive.

No queue buildup, no processing bottleneck, no container restarts detected between snapshots. The pipeline is in a clean idle state.

### PLC & Production State

**Line is stopped.** Redis DB1 (PLC state) is empty on both nodes -- no speed, no material position, no part presence, no tracking triggers. The PLC monitor containers are running and actively cycling their read/write loops (~30ms per cycle based on log timestamps), but reading empty/zero values from the PLC. Camera acquisition containers (ca-sis-surface 1-8) are running at steady ~0.10 CPU each, indicating they are in a polling/standby loop, not actively acquiring frames.

### GUI & Infrastructure Status

All web interfaces responding:

| Endpoint | Status | Note |
|----------|--------|------|
| vk01 Frontend | 200 | OK |
| vk01 Portainer | 200 | OK |
| vk01 Redis Insight | 200 | OK |
| vk01 RabbitMQ UI | 200 | OK |
| vk01 pgAdmin | 302 | Redirect (normal - login page) |
| vk01 Grafana | 302 | Redirect (normal - login page) |
| vk02 Frontend | 200 | OK |
| vk02 Portainer | 200 | OK |
| vk02 Redis Insight | 200 | OK |
| vk02 RabbitMQ UI | 200 | OK |
| vk02 pgAdmin | 302 | Redirect (normal) |
| vk02 Grafana | 302 | Redirect (normal) |
| vk03 Frontend | 200 | OK |
| vk03 Portainer | 200 | OK |
| vk03 Grafana | 302 | Redirect (normal) |

All GUIs healthy. The 302 redirects on pgAdmin and Grafana are expected authentication redirects.

### Changes Since Last Check

**Delta: 15 minutes (03:30 → 03:45 UTC)**

- **vk01 uptime**: 10,446s → 11,346s (+900s) -- normal tick, no reboot
- **vk02 uptime**: 19,175s → 20,090s (+915s) -- normal tick, no reboot
- **vk03 uptime**: 20,952s → 21,852s (+900s) -- normal tick, no reboot
- **vk01 GPU metrics**: Remain null (gpu_mem_pct, gpu_temp_c) -- blackout persists from previous snapshot
- **vk02 GPU temp**: 21°C → 23°C (+2°C) -- minor, stable
- **vk01 RAM**: 13.1% → 12.9% (-0.2%) -- slight decrease, cadvisor memory freed
- **vk03 RAM**: 25.8% → 26.3% (+0.5%) -- minor uptick, Prometheus memory grew 241MB → 283MB
- **vk01 Prometheus memory**: 794MB → 652MB (-142MB) -- TSDB compaction occurred
- **vk02 Prometheus memory**: 685MB → 644MB (-41MB) -- minor compaction
- **vk02 visualizer memory**: 315MB → 317MB (+2MB) -- slow leak continues
- **vk02 database-server**: 1,082MB → 1,082MB -- stable (previous growth stopped)
- **Error logs**: vk01 database-server errors now visible again (PIDs 32702/32709 at 03:09 UTC), confirming the 1/min probe cadence continues

No container restarts, no new containers, no removed containers. System is in steady idle state.

### Trending Concerns

Based on the 24h trend data (17 samples):

1. **vk01 disk at 81.6% -- highest in the cluster, flat but dangerously high**: No growth trend (stable at 81% for 24h) but leaves only ~19% headroom. When production resumes and image saving begins, this could fill rapidly. The 17% image disk is separate and has room, but root disk pressure affects PostgreSQL WAL, Docker layers, and system logs.

2. **GPU utilization flat at 0% for 24+ hours**: Models loaded consuming ~6GB VRAM per node with zero inference. Extended idle with loaded models is wasted thermal/power overhead.

3. **vk02 visualizer memory creep**: 315MB → 317MB in 15 minutes of idle. At ~8MB/hour idle rate, it will approach previous crash thresholds within 10-12 hours.

4. **vk01 GPU telemetry blackout persists**: Now at least 30+ minutes with null GPU memory and temperature readings. This is a new trend not present in earlier 24h samples where gpu_mem_pct was consistently 67%.

### Error Log Analysis

**Cross-service correlation:**

1. **`database "visionking" does not exist`** (vk01 + vk02 PostgreSQL, synchronized):
   - vk01: PIDs 32702/32709 at 03:09 UTC
   - vk02: PIDs 31173/31180 at 03:09 UTC (previous: 31173/31180 at 02:54 UTC)
   - Exact same timestamp on both nodes = single client probing both. Source: vk03 backends.

2. **vk02 `visionking-visualizer`** (gunicorn worker crashes):
   - Historical: Feb 4 (SIGKILL/OOM), Feb 10 (SIGKILL/OOM + KeyError), Feb 18 (exit code 1)
   - Pattern: Workers crash every ~6-8 days. Last crash was Feb 18 19:08, ~8.5 hours ago.
   - Current memory: 317MB and climbing.

3. **vk02 `dw-sis-surface`** (database writer):
   - PostgreSQL connection errors to `10.244.70.50:2345` (vk03?) -- "Network unreachable"
   - RabbitMQ connection drops and reconnects (Feb 17, Feb 18)
   - These correlate with the node reboot windows on Feb 17-18.

4. **vk02 `ct-sis-surface`** (camera trigger):
   - `sw::redis::IoError` exceptions -- Redis connection failures during reboot windows
   - "Falha ao setar IP1!" -- failed to set camera IP after Redis disconnect
   - These are transient boot-time issues, now resolved.

5. **PLC monitors** (vk01 `visionking-plc-monitor`, vk02 `pm-sis-surface`):
   - WARNING-level logs are normal operational cycling, not actual warnings.
   - ~50 lines per cycle at ~30ms intervals = excessive log volume but no errors.

No cascade failures detected. All current errors are either transient (post-reboot) or persistent nuisance (wrong DB name probe).

### Recommendations

**Immediate (fix now):**
- Verify vk01 GPU driver health: Run `nvidia-smi` on vk01 host via Portainer console. The GPU telemetry blackout (null metrics) must be resolved before production resumes. If the driver is in a fault state, the inference container won't function.

**Short-term (this week):**
- Fix the `visionking` database name in vk03 backend connection strings to eliminate the synchronized FATAL probe errors on vk01/vk02
- Add `--max-requests 1000 --max-requests-jitter 50` to vk02 `visionking-visualizer` gunicorn config to mitigate the memory leak before the next worker crash
- Address vk01 root disk at 81.6% -- identify and clean up unused Docker images, old logs, or stale data

**Long-term (architectural):**
- Standardize inference versions across nodes (vk01 on dev:c014, vk02 on prod:c009)
- Implement symmetric service deployment so nodes are interchangeable
- Reclassify PLC monitor log levels from WARNING to DEBUG

### NEW IMPROVEMENTS

1. **[WARNING] vk01 GPU telemetry blackout now sustained for 30+ minutes across two snapshot cycles** -- The null readings for `gpu_mem_pct` and `gpu_temp_c` on vk01 have persisted from the previous snapshot (03:30) through the current one (03:45), while `gpu_util_pct` still reports 0%. This is no longer a transient scrape failure -- the DCGM exporter has lost communication with the GPU driver. The Docker-level container stats still show `visionking-inference` at 5.97GB memory, meaning the container process is alive but its GPU access state is unknown. Before the next production shift: (a) check `docker logs nvidia-gpu-exporter` on vk01 for DCGM errors, (b) run `nvidia-smi` on the host to test driver responsiveness, and (c) if the driver is unresponsive, a host reboot may be required to recover the GPU.

2. **[INFO] vk03 Prometheus memory grew 17% in 15 minutes during idle (242MB → 283MB)** -- While vk01 and vk02 Prometheus instances compacted (both decreased), vk03's grew by 41MB. vk03 Prometheus is scraping only 10 containers (vs 27-29 on vk01/vk02), so this growth rate is disproportionate. Check whether vk03's Prometheus has different retention or scrape interval settings than the other nodes. If vk03's `scrape_interval` is shorter (e.g., 5s vs 15s), it would explain the faster ingestion despite fewer targets.

3. **[INFO] vk02 GPU temperature stabilized at 23°C -- no thermal cycling observed** -- Unlike vk01's previously reported 18-27°C oscillation, vk02's GPU temperature moved only 2°C (21→23°C) between snapshots. This differential behavior between identical hardware suggests vk01's fan control issue may be node-specific (BIOS fan curve, physical fan obstruction, or thermal paste degradation on vk01). When vk01's GPU telemetry recovers, compare fan RPM readings between nodes to isolate the cause.

4. **[SUGGESTION] vk02 `is-sis-surface` (image saver) container has accumulated 120MB of writable layer data** -- The container's `Size` field shows `120MB (virtual 295MB)`, up from `120MB (virtual 294MB)` in the previous snapshot. This writable layer growth during idle suggests the image saver is writing temporary files or logs inside the container filesystem rather than to mounted volumes. Over time this bloats the container's diff layer and slows `docker commit`/migration operations. Mount an external volume for any temp file operations, or add a periodic cleanup inside the container.
