

## VisionKing 03002 — Laminacao/Steel — Diagnostic Report

### SEVERITY: WARNING

### Executive Summary

The system is **operationally active** — the production line is running steel bars (F-series, ~20s intervals between bars) and the full camera-to-inference pipeline is processing on vk01. However, two persistent data-path issues are causing **data loss**: the `dw-sis-surface` database writer is failing on every message due to `bar_uuid='empty'`, and the `visionking-result-writer` is discarding frames for the same reason (`part_uuid=empty`). vk02 remains a ghost node (reachable but no containers running, Prometheus null), and vk01 root disk at 81.8% continues its slow climb toward critical.

### Node Status

| Node | CPU | RAM | Disk (root) | Disk (img) | GPU Util | GPU Mem | GPU Temp | Containers | Status |
|------|-----|-----|-------------|------------|----------|---------|----------|------------|--------|
| vk01 | 7.0% | 21.5% | **81.8%** | 24% | 0% | 67.4% | 36°C | 27 running | **WARNING** — disk, data-path errors |
| vk02 | null | null | null | 44% | null | null | null | 0 | **WARNING** — no containers, no metrics |
| vk03 | 0.4% | 25.2% | 25.7% | n/a | n/a | n/a | n/a | 10 running | HEALTHY |

### Pipeline Health

**Camera Acquisition** — All 4 `ca-sis-surface` containers are running and consuming consistent CPU (~0.22 each), confirming active frame capture from 4 cameras.

**Image Saver** — `is-sis-surface` is healthy (health endpoint confirms memory/rabbitmq/redis/storage all OK), running at 5.1GB memory (up from 5.5GB in previous snapshot — slight decrease, possibly due to frame cache turnover). CPU increased from 0.14 to 0.24, consistent with active production.

**RabbitMQ** — All 3 queues (`dw-sis-surface-queue`, `is-sis-surface-queue`, `rc-sis-surface-queue`) show **0 messages, 0 backlog, 1 consumer each**. Publish/deliver rates are 0.0 at the instant of snapshot, but delivery_tag numbers in the error logs (~1,800,000 range) confirm millions of messages have been processed. The zero queue depth means consumers are keeping up.

**Inference** — `visionking-inference` CPU jumped from 0.53 to 0.89 between snapshots, and GPU temp rose from 23°C to 36°C (+13°C). This confirms the inference engine is actively processing frames. GPU utilization reads 0% at the snapshot instant but the 24h trend shows spikes up to 86% — the inference runs in bursts between frames.

**Database Writers** — **CRITICAL DATA PATH FAILURE**: Both `dw-sis-surface` and `visionking-result-writer` are failing:
- `dw-sis-surface`: Repeated `ValueError: UUID 'empty' for field 'bar_uuid' cannot be reconstructed` — every batch message is rejected.
- `visionking-result-writer`: `Mensagem descartada do batch DB: UUID inválido. frame_uuid=<valid>, part_uuid=empty` — valid frame UUIDs are being paired with empty part UUIDs, causing all DB writes to be discarded.

**Root cause**: The PLC/tracking system is not populating `bar_uuid`/`part_uuid` into the message pipeline. The inference processes frames successfully but the results cannot be persisted to PostgreSQL because the bar identification is missing. This means **inspection results are being generated but thrown away**.

### PLC & Production State

**The line IS running.** Evidence:
- `visionking-result` is writing `'false'` (= no defect / pass) to Redis for sequential bar IDs: `F32331400601` through `F32331400804`, with ~20s intervals between bars. The F-series naming and sequential numbering confirm active steel bar production.
- `frame_current_speed` in discarded messages shows values like `2631.500000` (mm/min during rolling) and `0.000000` (between bars).
- `Peso_Tar_Tracking` values of `1973`-`1974` kg confirm material is on the line.
- The PLC monitor is cycling normally (~30ms per read/write/alarm cycle).

**However**: Redis DB1 (PLC state) shows **0 keys** at snapshot time, and the inference engine logs `Speed field 'Velocidade_Tratada' not found in Redis hash 'DB_Speed'`. This means the PLC-to-Redis bridge is either intermittent or the keys are being consumed/expired faster than the snapshot captures them. The missing speed field causes the inference to default to 100% frame sampling (no speed-based subsampling), which is safe but wasteful.

### GUI & Infrastructure Status

All web interfaces are responding:
- **200 (OK)**: vk01/vk02/vk03 frontends, Portainer instances, Redis Insight, RabbitMQ UI
- **302 (Redirect)**: PgAdmin and Grafana instances — normal behavior (redirect to login)
- No degradation from previous snapshot.

### Changes Since Last Check (2.5h delta)

| Metric | Previous (18:30 UTC) | Current (21:00 UTC) | Delta |
|--------|---------------------|---------------------|-------|
| vk01 uptime | 55,387s (15.4h) | 64,387s (17.9h) | +2.5h, no reboot |
| vk01 GPU temp | 23°C | **36°C** | **+13°C** (production active) |
| vk01 inference CPU | 0.53 | **0.89** | +68% (active inference) |
| vk01 camera CPUs | ~0.17 each | ~0.22 each | +29% (active capture) |
| vk01 `is-sis-surface` CPU | 0.14 | 0.24 | +71% (active saving) |
| vk01 `is-sis-surface` mem | 5,552MB | 5,109MB | -443MB (cache turnover) |
| vk01 `database-server` mem | 15,768MB | 16,068MB | +300MB (active writes) |
| vk01 Prometheus mem | 929MB | 982MB | +53MB (more metrics) |
| vk03 Prometheus mem | 476MB | 510MB | +34MB (growing) |
| vk03 uptime | 218,309s (2.5d) | 227,309s (2.6d) | +2.5h, no reboot |
| vk02 image saver health | ssh_error | ssh_error | No change — persistent |

**Key change**: System transitioned from **idle** (previous snapshots in existing improvements) to **active production**. All pipeline CPU metrics increased accordingly.

### Trending Concerns (24h)

1. **vk01 root disk at 81.8%** — Stable over 24h (min/max 81%) but already at the threshold established in prior reports. Active production will accelerate image and log accumulation.
2. **vk01 RAM 24h average 23.8% but spiked to 45%** — The max 45% spike correlates with production bursts. The `database-server` at 16GB and `visionking-inference` at 15.1GB together consume ~31GB, leaving limited headroom for image saver spikes.
3. **vk02 img_saved disk stuck at 44%** — No change over 24h. With no containers running on vk02, this is stale data from a previous deployment occupying nearly half the image storage partition.
4. **GPU utilization 0-86% range** — Bursty inference is normal for this pipeline architecture, but the 86% peaks suggest the model is GPU-memory-bound (67.4% VRAM constant) and occasionally compute-saturated during high frame rates.

### Error Log Analysis

**Cascade failure chain identified:**

```
PLC/Tracking → [bar_uuid not populated] → Redis DB1 empty
    ↓
Inference → processes frames → messages have part_uuid=empty
    ↓
dw-sis-surface → ValueError: UUID 'empty' for 'bar_uuid' → message rejected
    ↓
visionking-result-writer → UUID inválido, part_uuid=empty → batch discarded
    ↓
PostgreSQL → receives NO inspection results
```

The `visionking-result` service is the only one successfully writing — it posts `'false'` (pass/fail) to Redis for bar IDs, meaning it has access to bar identification through a different path (possibly direct PLC tag reading rather than the message pipeline). But the database persistence path is completely broken.

**Additional inference errors:**
- Frequent `File is empty` on `.bin` image files across all 4 cameras (DA3488406-409). These are race conditions where inference reads files the image saver hasn't finished writing. The retry mechanism (3 attempts, 0.2s backoff) handles this gracefully — these are warnings, not failures.
- One `No such file or directory` error — a file was deleted/moved between queue submission and inference read. Rare and not concerning.
- `Speed field 'Velocidade_Tratada' not found in Redis hash 'DB_Speed'` — the PLC speed data is not reaching Redis, forcing 100% sampling mode.

### Recommendations

**Immediate (fix now):**
1. **Investigate why `bar_uuid`/`part_uuid` are 'empty' in the message pipeline.** This is causing 100% data loss on inspection results to PostgreSQL. Check the `visionking-plc-monitor` → Redis → inference message chain. The PLC monitor logs show it's cycling normally but the bar UUID is not being injected into the frame metadata. This may be a configuration issue in the PLC tag mapping or a race condition where frames are queued before the bar tracking trigger fires.

2. **Verify Redis `DB_Speed` hash is being populated.** The missing `Velocidade_Tratada` field is both a symptom of the PLC-Redis sync issue and an efficiency problem (100% frame sampling wastes GPU).

**Short-term (this week):**
3. Investigate why vk02 has zero containers running despite being reachable. The ssh_error on health endpoints and null Prometheus metrics suggest vk02 was intentionally drained or suffered a Docker daemon issue.
4. Clean up the 44% stale image data on vk02's image partition.

**Long-term (architectural):**
5. Add graceful handling for `part_uuid=empty` in the database writers — either queue messages for retry when the UUID becomes available, or write them with a sentinel value for later reconciliation, rather than silently discarding.

---

### NEW IMPROVEMENTS

1. **[WARNING] `bar_uuid` and `part_uuid` are systematically 'empty' during active production, causing 100% inspection data loss to PostgreSQL** — Both `dw-sis-surface` (ERROR-level) and `visionking-result-writer` (WARNING-level) are rejecting every message because the bar/part UUID fields contain the literal string `'empty'`. This is happening while bars are actively being inspected (F32331400601-F32331400804). The `visionking-result` service CAN identify bars (it writes pass/fail to Redis by bar ID), but this identification is not reaching the database writer message path. Investigate the message enrichment step between inference output and database writer input — the component responsible for attaching `bar_uuid` to inference results is either misconfigured or the PLC tracking trigger that associates frames to bars is not firing for the database writer queue.

2. **[WARNING] Redis hash `DB_Speed` missing field `Velocidade_Tratada` — inference defaulting to 100% frame sampling** — The inference `batch_processor.py:233` logs that the speed field expected from PLC is absent, forcing it to process every frame rather than subsampling based on line speed. During active production at 2631 mm/min, this means significantly more GPU work than necessary. Verify the `visionking-plc-monitor` is writing speed data to the correct Redis DB and key name. The PLC monitor logs show `writeBar` completing successfully, but the data may be going to a different Redis database or key than what inference expects.

3. **[INFO] vk01 GPU recovered from previous telemetry blackout — now reporting 36°C and 67.4% memory** — The GPU null readings reported in the 00:33-00:48 UTC improvements have resolved. The GPU is now healthy and actively processing inference. The DCGM exporter recovered without intervention, likely after a driver-level transient. No immediate action needed, but this confirms the GPU driver on vk01 has an intermittent communication issue with DCGM worth monitoring.

4. **[INFO] vk01 `database-server` (PostgreSQL) memory grew 300MB in 2.5 hours during production (15.8GB → 16.1GB)** — This is the largest memory consumer on vk01 at 16GB. The growth during active production is expected (shared buffers, work_mem for queries), but at this trajectory it will consume significant host RAM over a full production shift. Monitor for OOM pressure on vk01 given inference (15.1GB) + PostgreSQL (16.1GB) already total 31.2GB.

5. **[SUGGESTION] Inference image loading race condition is pervasive across all 4 cameras** — The `File is empty` warnings appear for all camera IDs (DA3488406, DA3488407, DA3488408, DA3488409) at ~3-4 second intervals. While the 3-retry mechanism handles this, the frequency suggests a systematic timing issue: the image saver notifies inference via RabbitMQ before the `.bin` file is fully flushed to disk. Adding an `fsync` or a brief write-completion signal before publishing to the queue would eliminate these retries and reduce inference latency by ~0.2-0.4s per affected frame.

6. **[INFO] vk02 is fully dark — reachable at network level but running zero containers** — In the previous analysis cycle vk02 had running containers. It now shows empty container lists, null Prometheus host metrics, and persistent ssh_error on health endpoints. Only the RabbitMQ queues on vk02 still show consumers (likely connected from vk01's consumers to vk02's broker). This effectively makes the deployment single-node (vk01) for all processing, with no redundancy for cameras, inference, or database writing.
