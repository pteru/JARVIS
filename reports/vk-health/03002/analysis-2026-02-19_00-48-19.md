## VisionKing 03002 — Diagnostic Report

**Timestamp:** 2026-02-23 06:30 UTC | **Snapshot delta:** ~2h 15m (04:15 → 06:30 UTC)

---

### SEVERITY: WARNING

---

### Executive Summary

The system is in **idle/standby state** — all containers are running on vk01 and vk03, but the production line is stopped (0% GPU utilization, empty Redis caches, zero RabbitMQ throughput). vk02 remains fully dark with no containers. The persistent `bar_uuid='empty'` error pattern and image file race conditions from the last production shift are no longer active post-reboot, but will recur when production resumes unless root causes are addressed. vk01 root disk at **81.8%** remains the most pressing infrastructure concern.

---

### Node Status

| Node | CPU | RAM | Disk (root) | Disk (img) | GPU Util | GPU Mem | GPU Temp | Containers | Status |
|------|-----|-----|-------------|------------|----------|---------|----------|------------|--------|
| vk01 | 5.5% | 18.8% | **81.8%** | 24% | 0% | 67.4% | 24°C | 27 running | Idle, healthy |
| vk02 | n/a | n/a | n/a | 44% | n/a | n/a | n/a | 0 running | **Dark — no containers** |
| vk03 | 0.4% | 24.9% | 25.7% | n/a | n/a | n/a | n/a | 11 running | Idle, healthy |

---

### Pipeline Health

**Pipeline is fully idle.** No frames are flowing through the system.

- **RabbitMQ queues (vk01):** All 3 queues at 0 messages, 0 publish/deliver rate, 1 consumer each — healthy standby.
- **RabbitMQ queues (vk02):** Same 3 queues visible with consumers, despite vk02 having zero containers — these consumers are likely vk01 services connected to vk02's broker.
- **Redis DB0 (frame cache):** 0 keys on both vk01 and vk02 — no frames cached.
- **Redis DB1 (PLC):** Empty keys and values — PLC state has been flushed or was never populated this session.
- **Image saver health (vk01):** All subsystems healthy (memory, rabbitmq, redis, storage).
- **Image saver health (vk02):** `ssh_error` — cannot check.

**Container stability:** All vk01 containers show "Up 7 hours" (previously "Up 4 hours"), indicating no restarts in the interval. vk03 containers show "Up 3 days" (previously "Up 2 days") — also stable.

---

### PLC & Production State

**Line is STOPPED.**

- Redis DB1 is completely empty — no PLC tags, no speed data, no bar tracking state.
- The `visionking-plc-monitor` is still polling the PLC (readBar → redisRead → setBar cycle every ~50ms) but `writeBar` and `alarms` steps are absent, confirming no active bar is being tracked.
- No new bar IDs appearing in `visionking-result` logs (the last bars were F32391100101-103 from the 03:28 UTC production window).
- The missing `Velocidade_Tratada` key in Redis `DB_Speed` will cause inference to default to 100% sampling when production resumes.

---

### GUI & Infrastructure Status

| Endpoint | Status | Notes |
|----------|--------|-------|
| vk01 frontend | 200 | OK |
| vk01 portainer | 200 | OK |
| vk01 redis insight | 200 | OK |
| vk01 rabbitmq ui | 200 | OK |
| vk01 pgadmin | 302 | Redirect (normal — login page) |
| vk01 grafana | 302 | Redirect (normal — login page) |
| vk02 frontend | 200 | OK (served by vk02's nginx, but no backend data) |
| vk02 portainer | 200 | OK |
| vk02 redis insight | 200 | OK |
| vk02 rabbitmq ui | 200 | OK |
| vk02 pgadmin | 302 | Redirect (normal) |
| vk02 grafana | 302 | Redirect (normal) |
| vk03 frontend | 200 | OK |
| vk03 portainer | 200 | OK |
| vk03 grafana | 302 | Redirect (normal) |

All web interfaces are responding normally.

---

### Changes Since Last Check

| Metric | Previous (04:15) | Current (06:30) | Delta |
|--------|-----------------|-----------------|-------|
| vk01 uptime | 4.3h (15,626s) | 6.6h (23,711s) | +2.25h (no restart) |
| vk01 CPU | 6.5% | 5.5% | -1.0% (normal idle variation) |
| vk01 RAM | 18.6% | 18.8% | +0.2% (stable) |
| vk01 GPU temp | 27°C | 24°C | -3°C (cooling) |
| vk01 database-server mem | 2,996MB | **5,223MB** | **+2,227MB (+74%)** |
| vk01 is-sis-surface mem | 4,158MB | **10,729MB** | **+6,571MB (+158%)** |
| vk01 visionking-inference mem | 8,745MB | 8,829MB | +84MB (+1%) |
| vk01 visionking-visualizer mem | 358MB | 367MB | +9MB |
| vk01 prometheus mem | 810MB | 677MB | -133MB (compaction) |
| vk03 CPU | 8.9% | 0.4% | -8.5% (was likely a spike) |
| vk03 uptime | 2.9 days | 3.0 days | +2.25h (no restart) |

**Critical changes:**
1. **`is-sis-surface` (image saver) grew 6.5GB in 2.25 hours** — from 4.2GB to 10.7GB during what should be idle/standby. This is an alarming memory growth rate of ~2.9GB/hour with no production frames flowing.
2. **`database-server` (PostgreSQL) grew 2.2GB in 2.25 hours** — from 3.0GB to 5.2GB during idle, at ~1GB/hour. This mirrors the pre-reboot accumulation pattern.

---

### Trending Concerns (24h)

| Metric | Trend | Concern |
|--------|-------|---------|
| vk01 disk_root | Flat at 81.8% | Stable but dangerously high — any production shift adding images will push it higher |
| vk01 RAM | 13-18%, trending up | The image saver at 10.7GB is the driver; will hit host memory pressure if it continues |
| vk02 disk_img_saved | Flat at 44% | Healthy, but vk02 is non-functional |
| vk01 GPU | Stable at 67.4% / 0% util | Models loaded but idle — expected |
| vk03 metrics | All stable | Dashboard node is healthy |

---

### Error Log Analysis

**Cross-service correlation of active errors:**

1. **`visionking-result` (WARNING):** Writing `'false'` to Redis for bars F32391100101-103. These are the final bars from the last mini production run (~03:28 UTC). The `false` value is an inspection pass/fail result — this is normal operational output, not an error. The bars were processed and results were delivered.

2. **`visionking-result-writer` (WARNING):** Systematically discarding all messages due to `part_uuid=empty`. Every single database write batch was rejected. This is the **same root cause** as previous reports: the message enrichment pipeline is not attaching UUIDs. Delivery tags advanced from ~48k to ~160k, meaning ~112,000 messages were consumed from RabbitMQ and discarded in the interval between production runs.

3. **`dw-sis-surface` (ERROR):** `ValueError: UUID 'empty' for field 'bar_uuid' cannot be reconstructed`. Same root cause as #2 but this is the secondary database writer with stricter validation. Delivery tags at ~160k confirm it's processing the same message stream.

4. **`visionking-inference` (WARNING):** Image files are empty on first read attempt across all 4 cameras (DA3488406-409). All are Attempt 1/3 failures with successful retries. The race condition between image saver disk writes and inference reads persists. Additionally, `Velocidade_Tratada` field missing from `DB_Speed` hash — still unresolved.

5. **`visionking-plc-monitor` (WARNING):** Normal operational cycle logging. No actual errors — this is the verbose WARNING-level misclassification noted in previous reports.

**Cascade pattern:** The `empty` UUID issue flows: PLC monitor fails to write tracking context → inference processes frames without bar association → result writer receives messages with `part_uuid=empty` → both database writers reject all messages → **zero inspection data reaches PostgreSQL**.

---

### Recommendations

**Immediate (fix before next production shift):**
- Investigate `is-sis-surface` memory leak — 10.7GB and growing at 2.9GB/hour during idle. At this rate, it will consume all available host RAM within hours of production starting. This container needs a restart and a memory limit imposed via Docker.
- Verify `DB_Speed` Redis hash has the `Velocidade_Tratada` field populated before production starts, or the inference will run at 100% sampling (wasting GPU resources).

**Short-term (this week):**
- Root-cause the `bar_uuid='empty'` pipeline defect — this has persisted across multiple production runs and reboots. Until fixed, **no inspection data reaches the database**, making the system functionally blind from a traceability standpoint.
- Bring vk02 back online or document why it is intentionally dark. The deployment is currently single-node with no processing redundancy.
- Set PostgreSQL `shared_buffers` ceiling and `idle_in_transaction_session_timeout` to prevent the 5GB+ idle memory accumulation.

**Long-term (architectural):**
- Add `fsync` or write-completion signaling before the image saver publishes to RabbitMQ, eliminating the pervasive empty-file retry pattern.
- Reduce PLC monitor log level from WARNING to DEBUG for operational cycle messages.
- Address vk01 root disk at 81.8% — implement automated image cleanup or expand storage.

---

### NEW IMPROVEMENTS

1. **[WARNING] `is-sis-surface` (image saver) is leaking ~2.9GB/hour during IDLE — 10.7GB and growing with zero production load** — This container grew from 4.2GB to 10.7GB (+6.5GB) in the 2.25 hours between snapshots, with no frames being processed (Redis DB0 has 0 keys, RabbitMQ has 0 publish rate). This is worse than the previous production-shift leak (which was ~4.5GB over a full shift) — the idle leak rate now exceeds the production leak rate. The container's writable layer also grew from 114MB to 125MB. This suggests a background process inside the image saver (possibly a monitoring thread, health check response accumulator, or unclosed file handles from the previous production run) is continuously allocating memory. Restart the container immediately and impose a `mem_limit: 4g` in its compose file. Then instrument the container with `tracemalloc` or `/proc/PID/smaps` analysis to identify the leaking allocation path.

2. **[WARNING] vk01 PostgreSQL resumed aggressive idle memory growth — 3.0GB to 5.2GB (+74%) in 2.25 hours with zero active connections** — Despite the reboot 7 hours ago resetting PostgreSQL to 1.2GB, it has already climbed to 5.2GB with `active_connections: 0`. The previous cycle saw it reach 16.7GB before reboot. At the current growth rate (~1GB/hour), it will return to 16GB within 11 hours. The `shared_buffers` recommendation from previous reports has not been implemented. Additionally, the periodic "database visionking does not exist" probes (confirmed at 1/minute from vk03) are spawning and killing backend processes, which may be contributing to memory fragmentation. Prioritize setting `shared_buffers = 2GB` and `effective_cache_size = 4GB` in `postgresql.conf`.

3. **[INFO] vk01 `visionking-inference` memory baseline established post-reboot — 8.8GB is the warm idle footprint** — The inference container settled at 8.8GB after startup (up from 5.97GB cold start seen immediately post-reboot, to 8.7GB at the previous snapshot, to 8.8GB now). This +84MB growth over 2.25 hours of idle is minimal and expected (model warm-up, CUDA context). The previous production shift inflated it to 15.1GB, meaning ~6.3GB was runtime accumulation. Monitor whether it stays at ~8.8GB through idle, and track the growth rate once production resumes to distinguish model warm-up from actual leaks.

4. **[INFO] `visionking-visualizer` memory growing 4MB/hour during idle (358MB → 367MB)** — This confirms the idle-path memory leak previously reported for vk02's visualizer also exists on vk01. At 4MB/hour, it will reach the ~400MB OOM-crash threshold observed on vk02 in approximately 8 hours. The `--max-requests` gunicorn fix has still not been deployed. Both nodes' visualizer containers will likely need restarts during the next production shift.

5. **[SUGGESTION] RabbitMQ delivery tags on `dw-sis-surface` jumped from ~48k to ~160k — 112,000 messages consumed and discarded in one production window** — This quantifies the data loss from the `bar_uuid='empty'` bug: approximately 112,000 inspection result messages were pulled from the queue and thrown away. At ~8 messages per batch and ~4 cameras, this represents roughly 28,000 frame-level inspection results that were computed by GPU inference but never persisted. For a production traceability system in steel manufacturing, this is a compliance risk. Consider adding a dead-letter queue so discarded messages are retained for later reprocessing once the UUID enrichment bug is fixed, rather than being permanently lost.
