## VisionKing 03002 Health Report — 2026-02-19 02:43 UTC

### SEVERITY: WARNING

### Executive Summary
The deployment is **idle and stable** with no production activity. All 3 nodes are reachable, all containers are running, and all web UIs are responding. The persistent issues remain unchanged: vk01 disk at 82%, the `"visionking" database does not exist` errors continuing on both processing nodes, and the vk02 visualizer memory leak slowly accumulating. No new failures or degradation detected in this 10-minute window.

### Node Status

| Node | CPU | RAM | Disk (root) | Disk (img) | GPU Util | GPU Mem | GPU Temp | Uptime | Containers | Status |
|------|-----|-----|-------------|------------|----------|---------|----------|--------|------------|--------|
| vk01 | 3.0% | 13.2% | **81.6%** | 17% | 0% | 67.4% | 24°C | 2.1h | 28 running | WARNING (disk) |
| vk02 | 4.6% | 14.5% | 16.9% | 41% | 0% | 67.4% | 27°C | 4.5h | 29 running | OK |
| vk03 | 0.5% | 26.3% | 25.6% | N/A | N/A | N/A | N/A | 5.0h | 10 running | OK |

### Pipeline Health
- **Queues**: All 6 RabbitMQ queues (3 per node) at 0 messages, 0 publish/deliver rate, 1 consumer each — pipeline idle but connected.
- **Image Saver**: Healthy on both nodes (memory, rabbitmq, redis, storage all green).
- **Inference**: Models loaded (67.4% GPU mem) but 0% utilization — idle, ready state.
- **Database Writers**: Near-zero CPU on both nodes — idle but running.
- **No queue buildup or backpressure.**

### PLC & Production State
- **Line is NOT running.** Redis DB1 (PLC state) is empty on both nodes.
- All Redis databases (DB0-DB3) empty. No cameras active, no material on line, no triggers firing.
- PLC monitors actively polling (~30ms cycles) but reading empty state.
- Consistent with late-night idle (02:43 UTC = 23:43 BRT).

### GUI & Infrastructure Status
All 15 monitored endpoints responding normally:
- **200 OK**: All frontends, Portainer instances, Redis Insight, RabbitMQ UI
- **302 Redirect** (auth redirects): Grafana, pgAdmin — normal behavior

### Changes Since Last Check (10-minute delta)
- **vk01 Prometheus memory**: 634MB → 768MB (+134MB, pre-compaction accumulation)
- **vk01/vk02 database-server errors**: +2 each (`"visionking" does not exist`, ~1/min rate confirmed)
- **vk02 GPU temp**: 28°C → 27°C (nighttime cooling)
- **vk02 visualizer memory**: 334MB → 316MB (-18MB, likely silent worker recycle)
- **No container restarts or state changes.**

### Trending Concerns (24h)
- **vk01 root disk at 81.6%** — Flat but only ~19% headroom remaining.
- **GPU memory at 67.4% with 0% utilization for 24h** — models loaded but unused.
- **vk01 CPU spikes to 7%** occasionally — likely scrape cycles.

### Error Log Analysis

**PostgreSQL `"visionking" does not exist`** (vk01 + vk02):
- Rate confirmed: 1 probe/minute per node, synchronized. Source: vk03 backends.

**vk02 `visionking-visualizer`**:
- Currently 316MB after 18MB drop (silent worker recycle). Memory leak continues.

**vk02 `dw-sis-surface`**:
- Historical PostgreSQL/RabbitMQ connection failures during reboot windows. Currently stable.

**PLC monitors**:
- WARNING-level operational noise continues at ~170 lines/sec.

### Recommendations

**Immediate:** None — system is idle and stable.

**Short-term:**
- Fix `"visionking"` database name in vk03 backend connection strings.
- Add `--max-requests` to vk02 visualizer gunicorn config.
- Free vk01 root disk space.

**Long-term:**
- Align inference versions between nodes.
- Standardize Docker Compose versions.
- Reclassify PLC monitor logs from WARNING to DEBUG.

### NEW IMPROVEMENTS

1. **[INFO] vk02 visualizer memory dropped 18MB in 10 minutes during idle** — Memory went from 334MB to 316MB without production load, suggesting a silent worker recycle not captured in error logs. Add `--access-logfile -` and `--error-logfile -` flags to gunicorn to capture worker lifecycle events.

2. **[INFO] PostgreSQL "visionking" probe rate confirmed at exactly 1 per minute per node** — 2 new FATAL entries per 10-minute delta on both nodes simultaneously. Source is definitively a 1-minute interval health check from vk03 backends. Check NestJS TypeORM/Prisma config for `database: "visionking"` setting.

3. **[SUGGESTION] vk01 Prometheus memory oscillation is widening** — Ingestion at 13.4MB/min (634MB → 768MB in 10 min). During production, peak could exceed 1GB. Consider `--storage.tsdb.max-block-duration=2h` to trigger more frequent compactions on disk-constrained vk01.
