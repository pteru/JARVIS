## VisionKing 03002 (Laminacao/Steel) — System Analysis Report

**Timestamp:** 2026-02-19T03:30:01Z | **Snapshot Delta:** 15 minutes

---

### SEVERITY: WARNING

---

### Executive Summary

The system is idle with all three nodes reachable and all containers running. The pipeline is completely inactive — zero Redis keys, zero RabbitMQ throughput, zero GPU utilization, and zero PostgreSQL connections. The primary concerns remain the persistent `database "visionking" does not exist` errors on vk01/vk02, the vk01 disk at 81.6%, and a notable change this snapshot: **vk01's GPU metrics (memory and temperature) have gone null**, suggesting the GPU exporter lost contact with the GPU driver.

---

### Node Status

| Node | CPU | RAM | Disk (root) | Disk (img) | GPU Util | GPU Mem | GPU Temp | Containers | Uptime | Status |
|------|-----|-----|-------------|------------|----------|---------|----------|------------|--------|--------|
| vk01 | 2.9% | 13.1% | **81.6%** | 17% | 0% | **null** | **null** | 27 running | 2.9h | WARNING |
| vk02 | 4.6% | 14.6% | 16.9% | 41% | 0% | 67.4% | 21°C | 29 running | 5.3h | OK |
| vk03 | 0.4% | 25.8% | 25.6% | N/A | N/A | N/A | N/A | 10 running | 5.8h | OK |

---

### Pipeline Health

**Status: FULLY IDLE**

- **Cameras (ca-sis-surface 1-8):** Running on both nodes with minimal CPU (~0.10 each), indicating idle polling but no frame acquisition.
- **Redis DB0 (frame cache):** 0 keys on both nodes — no frames in transit.
- **RabbitMQ:** All 6 queues (3 per node) at 0 messages, 0 publish/deliver rate. Each has 1 consumer connected and healthy.
- **Inference:** Containers running but 0% GPU utilization. Models remain loaded in VRAM (67.4% on vk02; **unknown on vk01** — see GPU section).
- **Database Writers:** `dw-sis-surface` near-zero CPU on both nodes. `visionking-result-writer` on vk01 similarly idle.
- **Image Saver:** Health endpoints report healthy on both nodes (Redis, RabbitMQ, storage, memory all green).

No queue buildup, no processing bottleneck. Pipeline is simply not receiving production triggers.

---

### PLC & Production State

**Line Status: STOPPED / IDLE**

- **Redis DB1 (PLC state):** Empty on both nodes — zero keys, zero values.
- **Redis DB2 (camera config):** 0 keys.
- **Redis DB3 (settings):** Empty.

Both PLC monitors (`visionking-plc-monitor` on vk01, `pm-sis-surface` on vk02) are actively cycling through their read/write loops (visible in WARNING-level logs), but there is no PLC data being stored in Redis. This indicates:
- PLC communication is active (the monitors are reading and writing PLC tags)
- The line is not running (no speed, material, or trigger data)
- Camera triggers are not firing

---

### GUI & Infrastructure Status

| Endpoint | Status | Note |
|----------|--------|------|
| vk01 Frontend | 200 | OK |
| vk01 Portainer | 200 | OK |
| vk01 Redis Insight | 200 | OK |
| vk01 RabbitMQ UI | 200 | OK |
| vk01 pgAdmin | 302 | Redirect (normal — login page) |
| vk01 Grafana | 302 | Redirect (normal — login page) |
| vk02 Frontend | 200 | OK |
| vk02 Portainer | 200 | OK |
| vk02 Redis Insight | 200 | OK |
| vk02 RabbitMQ UI | 200 | OK |
| vk02 pgAdmin | 302 | Redirect (normal) |
| vk02 Grafana | 302 | Redirect (normal) |
| vk03 Frontend | 200 | OK |
| vk03 Portainer | 200 | OK |
| vk03 Grafana | 302 | Redirect (normal) |

All web interfaces are responding normally. The 302s are expected authentication redirects.

---

### Changes Since Last Check (15-minute delta)

1. **vk01 GPU metrics went NULL** — Previous snapshot showed `gpu_mem_pct: 67.4%`, `gpu_temp_c: 27°C`. Current snapshot shows `gpu_mem_pct: null`, `gpu_temp_c: null`. GPU utilization remains at 0%. This is a significant change — the DCGM/nvidia-gpu-exporter may have lost connection to the GPU driver, or the GPU itself may have entered a low-power state that makes metrics unavailable.

2. **vk01 RAM increased slightly** — 13.06% to 13.13% (+0.07%), normal fluctuation.

3. **vk01 Prometheus memory grew** — 710MB to 794MB (+84MB), consistent with the documented pre-compaction ingestion cycle.

4. **vk02 RAM decreased** — 14.85% to 14.56% (-0.29%), likely PostgreSQL releasing cached pages.

5. **vk02 database-server memory dropped** — 1,094MB to 1,082MB (-12MB), reversing the previous growth trend. Autovacuum likely completed.

6. **vk02 GPU temperature rose** — 20°C to 21°C (+1°C), minor and within normal range.

7. **vk03 RAM decreased slightly** — 25.87% to 25.77%, normal fluctuation.

8. **PostgreSQL errors persisting** — Both vk01 and vk02 logged new `database "visionking" does not exist` FATAL entries (vk01: PIDs 14518/14525 at 03:16:54; vk02: PIDs 31173/31180 at 02:54:01), confirming the ~1/minute probe cadence continues.

---

### Trending Concerns (24h)

1. **vk01 disk at 81.6% — stable but dangerously high** — No movement in 24h (consistently 81%), but this leaves <20% headroom. Any production run generating saved images or WAL growth could push this toward critical.

2. **GPU utilization flat at 0% for 24h** — Models loaded, no work being done. Extended idle with models in VRAM is wasteful but not harmful if the line is expected to resume.

3. **vk01 GPU metrics now NULL** — New this snapshot. Could indicate the beginning of a GPU exporter failure or driver issue. Needs monitoring.

4. **vk02 visualizer CPU remains elevated at 0.48** — Consistently 6x higher than vk01's 0.08. Previously documented, unchanged.

---

### Error Log Analysis

**vk01:**
- `database-server`: `FATAL: database "visionking" does not exist` — recurring every ~15 min, 2 connections per probe. Source: vk03 backend(s).
- `visionking-plc-monitor`: WARNING-level operational logs only (readBar/setBar cycles). Not actual errors.

**vk02:**
- `database-server`: Same `visionking` database probe as vk01.
- `visionking-visualizer`: Historical OOM/worker crashes (Feb 4, 10, 18). Most recent: Feb 18 19:08. Currently stable at 315MB.
- `dw-sis-surface`: PostgreSQL connection failures to `10.244.70.50:2345` (Network unreachable) — historical entries from Feb 4 and a future-dated Feb 21 entry (clock issue previously documented). Also RabbitMQ reconnection events from Feb 17-18.
- `ct-sis-surface`: Redis IoError and "Falha ao setar IP1!" — correlated with reboot events (Feb 4, Feb 18). Recovered.
- `sm-sis-surface` / `mongo`: Docker daemon log stream corruption (`invalid character '\x00'`). Not service errors — Docker log driver issue.
- `pm-sis-surface`: WARNING-level operational logs only.

**vk03:**
- No error logs. Clean.

**Cross-correlation:** The `dw-sis-surface` PostgreSQL and RabbitMQ errors on vk02 align with the Feb 18 reboot window (~18:48-19:08). The `ct-sis-surface` Redis errors also correlate. These are all post-reboot transient connectivity failures that self-healed. No cascading failures detected in the current state.

---

### Recommendations

**Immediate (fix now):**
- Investigate vk01 GPU metrics going null — check `nvidia-smi` output and the `nvidia-gpu-exporter` container logs on vk01. If the DCGM exporter crashed or the GPU driver unloaded, this needs remediation before production.

**Short-term (this week):**
- Fix the `database "visionking"` connection string on the vk03 backend(s). This generates 4 FATAL log entries per minute across the cluster.
- Address vk01 disk at 81.6% — identify and clean up unnecessary files, old images, or Docker artifacts.
- Add `--max-requests 1000 --max-requests-jitter 50` to vk02's `visionking-visualizer` gunicorn config to prevent OOM recurrence.

**Long-term (architectural):**
- Standardize inference versions across vk01 (dev c014) and vk02 (prod c009).
- Implement GPU idle unloading policy for extended downtime periods.
- Reclassify PLC monitor operational logs from WARNING to DEBUG.

---

### NEW IMPROVEMENTS

1. **[WARNING] vk01 GPU metrics (memory, temperature) have gone null — potential GPU exporter or driver failure** — Between the 03:15 and 03:30 snapshots, vk01's `gpu_mem_pct` dropped from 67.4% to null and `gpu_temp_c` from 27°C to null, while `gpu_util_pct` remains at 0%. This indicates the `nvidia-gpu-exporter` (DCGM exporter) on vk01 can no longer read GPU telemetry. Check `docker logs nvidia-gpu-exporter` on vk01 for DCGM errors, and run `nvidia-smi` directly on the host to verify the GPU driver is still responsive. If the driver has entered a fault state, the inference container (which is still reporting 5.97GB memory usage via Docker stats) may also be unable to run inference when production resumes. This should be verified before the next shift.

2. **[INFO] vk01 and vk02 database-server error log PIDs are advancing, confirming active probing** — vk01 PIDs jumped from 13168/13175 (03:01 UTC) to 14518/14525 (03:16 UTC), a delta of ~1350 PIDs in 15 minutes. Each probe spawns 2 short-lived PostgreSQL backend processes. This PID advancement rate (~90 PIDs/min) is dominated by the `visionking` database probes. While PostgreSQL handles this gracefully, the PID churn adds unnecessary entries to `pg_stat_activity` history and bloats CSV logs if enabled. Fixing the connection string on vk03 will eliminate this overhead.

3. **[INFO] vk01 cadvisor memory dropped significantly — 195MB to 168MB (-27MB)** — This 14% reduction in cadvisor's memory footprint may indicate cadvisor performed an internal cache eviction or garbage collection. Given that cadvisor is scraping 27 containers on vk01, its 168MB footprint is reasonable but worth watching — cadvisor memory growth on large container counts is a known issue. If it climbs back above 200MB, consider adding `--docker_only=true` and `--disable_metrics=referenced_memory,cpu_topology,resctrl,hugetlb` flags to reduce its collection scope.
