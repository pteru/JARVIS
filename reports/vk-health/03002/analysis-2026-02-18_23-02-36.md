

# VisionKing 03002 Health Report — 2026-02-19 01:59 UTC

### SEVERITY: WARNING

### Executive Summary

The deployment is **stable but idle** — all 3 nodes are reachable, all containers are running, and no queues are backing up. The production line is **not running** (all Redis DBs empty, 0 queue throughput, 0% GPU utilization). The previously reported vk03 backend Redis connection issue appears to have **self-resolved** (no new errors in this snapshot), and vk01 has stabilized after its earlier reboot (uptime now ~83 min). The critical concern remains **vk01 disk at 81.6%** with zero change over 24h, suggesting a large static allocation rather than active growth — but still dangerously close to causing issues if production resumes.

### Node Status

| Node | CPU | RAM | Disk | GPU Util | GPU Mem | GPU Temp | Containers | Uptime | Status |
|------|-----|-----|------|----------|---------|----------|------------|--------|--------|
| vk01 | 3.0% | 13.1% | **81.6%** | 0% | 67.4% | 19°C | 28 running | 1h 23m | WARNING (disk) |
| vk02 | 4.7% | 14.6% | 16.9% | 0% | 67.4% | 21°C | 28 running | 3h 49m | HEALTHY |
| vk03 | 0.5% | 26.7% | 25.6% | N/A | N/A | N/A | 10 running | 4h 19m | HEALTHY |

### Pipeline Health

**Pipeline is idle.** No production data flowing:
- All RabbitMQ queues (6 total across vk01/vk02): **0 messages, 0 publish/deliver rate**
- All Redis caches: **0 keys** across DB0 (frame cache), DB2 (camera), DB3 (settings)
- Redis DB1 (PLC): **empty** on both nodes — no PLC tags being written
- GPU utilization: **0%** on both nodes (models loaded at 67.4% VRAM but not inferencing)
- Image saver health endpoints: **healthy** on both nodes (all dependencies OK)

**No queue backlog = no processing bottleneck**, but also no production activity.

**Container stability:** All containers have been running cleanly since the reboot window (~18:48-20:48 UTC). The `intel-gpu-exporter` that was crash-looping on vk01 in the previous snapshot is **no longer present** in the current container list — it appears to have been removed, which is the correct action since these are NVIDIA GPU nodes.

### PLC & Production State

**The production line is OFF.** Evidence:
- Redis DB1 (PLC state) is completely empty on both vk01 and vk02
- Zero keys means the PLC monitor is not writing any tags (speed, position, triggers)
- The `visionking-plc-monitor` container on vk01 is actively cycling through its read/set/write loop (per logs) but the PLC data block appears to contain no active production values
- `pm-sis-surface` on vk02 shows the same idle cycling pattern

This is consistent with nighttime/off-shift operation (snapshot at ~02:00 local time BRT).

### GUI & Infrastructure Status

| Endpoint | Status | Notes |
|----------|--------|-------|
| vk01 Frontend | 200 | OK |
| vk01 Portainer | 200 | OK |
| vk01 Redis Insight | 200 | OK |
| vk01 RabbitMQ UI | 200 | OK |
| vk01 PgAdmin | 302 | OK (redirect to login) |
| vk01 Grafana | 302 | OK (redirect to login) |
| vk02 Frontend | 200 | OK |
| vk02 Portainer | 200 | OK |
| vk02 Redis Insight | 200 | OK |
| vk02 RabbitMQ UI | 200 | OK |
| vk02 PgAdmin | 302 | OK (redirect to login) |
| vk02 Grafana | 302 | OK (redirect to login) |
| vk03 Frontend | 200 | OK |
| vk03 Portainer | 200 | OK |
| **vk03 PgAdmin** | **0** | **DOWN** (persistent) |
| vk03 Grafana | 302 | OK (redirect to login) |
| **vk01 Visualizer** | **0** | DOWN (known) |
| **vk02 Visualizer** | **0** | DOWN (known) |

### Changes Since Last Check

**Delta: ~71 minutes between snapshots (00:47 → 01:59 UTC)**

1. **vk01 uptime: 711s → 5016s** — Node stabilized after reboot, now at 83 min uptime. All services running cleanly.
2. **vk01 `intel-gpu-exporter` removed** — Was in `restarting` state in previous snapshot, now absent from container list. Someone (or automation) cleaned it up.
3. **vk01 GPU temp dropped: 26°C → 19°C** — Consistent with idle GPU cooling down after reboot thermal spike.
4. **vk02 GPU temp dropped: 24°C → 21°C** — Slight cooling, normal nighttime pattern.
5. **vk03 backend Redis errors resolved** — Previous snapshot showed persistent `ECONNREFUSED`/`EHOSTUNREACH` to `10.244.70.26:4000`. Current snapshot shows no new errors for vk03 backend. The connection appears to have recovered.
6. **vk01 RAM: 12.9% → 13.1%** — Minor increase, containers settling post-reboot.
7. **vk02 visualizer RAM: 414MB → 315MB** — Dropped ~100MB, suggesting a worker recycled (consistent with the known OOM pattern — a worker died and respawned with less accumulated memory).
8. **vk01 database-server RAM: 928MB → 976MB** — PostgreSQL buffer cache growing, normal behavior.

### Trending Concerns

Based on 24h trends (7 samples):

1. **vk01 disk at 81% — flat but dangerously high.** The 24h trend shows min=81, max=81 — no growth but no reduction either. This is a ~50TB static allocation (likely database or saved images). If production resumes and new data starts writing, this could cross 90% quickly.

2. **GPU memory at 67.4% constant across both nodes** — Models are loaded and stable. No memory leak pattern. This is expected steady-state for ONNX inference models.

3. **vk03 RAM at 25-27% (highest of all nodes)** — Despite being the lightest node (dashboard-only), it has the highest RAM percentage. The 5.78GB PostgreSQL data directory and the `database-server` container (111MB resident) are the primary consumers. The pgAdmin container at 309MB is also notable.

4. **CPU trends are flat and healthy** — vk01 2-7%, vk02 steady at ~4.7%, vk03 at ~0.5%. No concerning upward trends.

### Error Log Analysis

**Current snapshot errors (new since previous):**

| Service | Node | Error | Severity |
|---------|------|-------|----------|
| `visionking-plc-monitor` | vk01 | PLC read/write cycle warnings | INFO — Normal operational logging |
| `pm-sis-surface` | vk02 | PLC read/write cycle warnings | INFO — Normal operational logging |
| `visionking-visualizer` | vk02 | Worker OOM kills (Feb 4, 10, 18) | WARNING — Recurring pattern |
| `dw-sis-surface` | vk02 | PostgreSQL unreachable at 10.244.70.50:2345 | WARNING — Network issue during reboots |
| `ct-sis-surface` | vk02 | Redis IoError + "Falha ao setar IP1" | WARNING — Redis disconnect during reboots |
| `sm-sis-surface` | vk01/vk02 | Log stream corruption (`\x00`) | INFO — Docker log driver issue |

**Cross-correlation analysis:**
- The `dw-sis-surface` on vk02 has a **future timestamp** (`2026-02-21 18:00:01`) in its PostgreSQL connection error, confirming the clock drift issue noted in the previous report. This timestamp is ~3 days ahead.
- The `ct-sis-surface` Redis IoErrors on vk02 correlate with the reboot window (18:48 UTC) — the C++ service throws `sw::redis::IoError` when Redis drops during restart, which is expected but the service should handle it more gracefully.
- No **new** cascade failures detected. All services recovered after the reboot window.

### Recommendations

**Immediate:**
- None required — system is stable and idle. Production can resume safely.

**Short-term (this week):**
- Investigate and reclaim disk space on vk01 (81.6%). Identify what's consuming the space (`du -sh /home/vk01/Downloads/*/` via Portainer console). Old Docker images, saved inspection images, or database WAL files are the likely culprits.
- Fix NTP/clock on vk02 — the `dw-sis-surface` future timestamps (Feb 21) confirm clock drift. Run `timedatectl` and verify NTP sync.
- Fix vk03 pgAdmin (status 0) — container is running but port 5050 isn't responding. Check if the container is in a degraded state.

**Long-term (architectural):**
- Add `--max-requests 1000 --max-requests-jitter 50` to `visionking-visualizer` gunicorn configs on both nodes to prevent OOM worker kills.
- Implement proper Redis reconnection with exponential backoff in `ct-sis-surface` (C++) and `visionking-backend` (NestJS) to handle graceful recovery from infrastructure restarts.

### NEW IMPROVEMENTS

1. **[INFO] `intel-gpu-exporter` was removed from vk01 between snapshots** — This crash-looping container (searching for non-existent Intel i915 GPU on an NVIDIA machine) has been cleaned up. Verify it was also removed from the `docker-compose.yml` on vk01 so it doesn't return on next reboot. Also check vk02 — it was still crash-looping there in the previous snapshot but is absent from the current container *metrics* (though not confirmed removed from Docker).

2. **[SUGGESTION] PLC monitor WARNING log level is too verbose for normal operation** — Both `visionking-plc-monitor` (vk01) and `pm-sis-surface` (vk02) emit ~50 WARNING-level log lines per PLC cycle (~300ms). This is normal operational flow logging (`Inicio readBar`, `Fim readBar`, etc.) misclassified as WARNING. At ~170 log lines/second, this generates unnecessary log volume. Reclassify these to DEBUG level and reserve WARNING for actual anomalies.

3. **[INFO] vk02 `visionking-visualizer` memory dropped 100MB between snapshots (414MB → 315MB)** — This confirms the OOM recycling pattern: a worker accumulated memory, was killed, and respawned lighter. The gunicorn `--max-requests` recommendation from the previous report should be prioritized before the next production shift.

4. **[SUGGESTION] vk03 `database-server` container has 5.78GB disk footprint** — The PostgreSQL data volume on vk03 (`downloads_data...`) is growing. At 25.6% disk usage this isn't critical, but vk03 has no automated cleanup. Consider implementing WAL archiving or `pg_dump` rotation to prevent vk03 from following vk01's disk trajectory.
